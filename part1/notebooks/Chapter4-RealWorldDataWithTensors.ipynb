{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to represent different types of data with tensors. Types covered: images, tabular, time series, text.\n",
    "\n",
    "## Working with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 855, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "img_arr = imageio.imread(\"../data/p1ch4/image-dog/bobby.jpg\")\n",
    "img_arr.shape # H x W x C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1280, 855])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "img = torch.from_numpy(img_arr)\n",
    "out = img.permute(2, 0, 1) # Permute to fit C x H x W dimension ordering\n",
    "out.shape\n",
    "\n",
    "# Note: permute doesn't create a new image, but alters the size and stride information at the level\n",
    "# of the original tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To creat a dataset of multiple images to use as input for our neural networks, we store the images\n",
    "in a batch along the first dimension to obtain an N x C x H x W tensor.\n",
    "\"\"\"\n",
    "\n",
    "# An efficient way to create a batch is pre-allocation followed by loading from a directory\n",
    "\n",
    "batch_size = 3\n",
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '../data/p1ch4/image-cats/'\n",
    "filenames = [name for name in os.listdir(data_dir)\n",
    "             if os.path.splitext(name)[-1] == \".png\"] # Condition ensures images used are of a desired format.\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "\n",
    "    img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "    img_t = torch.from_numpy(img_arr)\n",
    "    img_t = img_t.permute(2, 0, 1)\n",
    "    img_t = img_t[:3]\n",
    "    batch[i] = img_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 90,  91,  93,  ..., 191, 191, 191],\n",
       "         [ 91,  91,  93,  ..., 191, 191, 191],\n",
       "         [ 91,  92,  93,  ..., 192, 192, 192],\n",
       "         ...,\n",
       "         [206, 210, 213,  ..., 220, 219, 218],\n",
       "         [209, 214, 214,  ..., 221, 220, 219],\n",
       "         [212, 212, 212,  ..., 219, 218, 218]],\n",
       "\n",
       "        [[108, 109, 111,  ..., 201, 201, 201],\n",
       "         [109, 109, 111,  ..., 201, 201, 201],\n",
       "         [109, 110, 111,  ..., 202, 202, 202],\n",
       "         ...,\n",
       "         [198, 202, 205,  ..., 214, 213, 212],\n",
       "         [201, 206, 206,  ..., 213, 212, 211],\n",
       "         [204, 204, 204,  ..., 211, 210, 210]],\n",
       "\n",
       "        [[120, 121, 123,  ..., 210, 210, 210],\n",
       "         [121, 121, 123,  ..., 210, 210, 210],\n",
       "         [121, 122, 123,  ..., 211, 211, 211],\n",
       "         ...,\n",
       "         [198, 202, 205,  ..., 214, 213, 212],\n",
       "         [201, 206, 206,  ..., 213, 212, 211],\n",
       "         [204, 204, 204,  ..., 211, 210, 210]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3529, 0.3569, 0.3647,  ..., 0.7490, 0.7490, 0.7490],\n",
       "         [0.3569, 0.3569, 0.3647,  ..., 0.7490, 0.7490, 0.7490],\n",
       "         [0.3569, 0.3608, 0.3647,  ..., 0.7529, 0.7529, 0.7529],\n",
       "         ...,\n",
       "         [0.8078, 0.8235, 0.8353,  ..., 0.8627, 0.8588, 0.8549],\n",
       "         [0.8196, 0.8392, 0.8392,  ..., 0.8667, 0.8627, 0.8588],\n",
       "         [0.8314, 0.8314, 0.8314,  ..., 0.8588, 0.8549, 0.8549]],\n",
       "\n",
       "        [[0.4235, 0.4275, 0.4353,  ..., 0.7882, 0.7882, 0.7882],\n",
       "         [0.4275, 0.4275, 0.4353,  ..., 0.7882, 0.7882, 0.7882],\n",
       "         [0.4275, 0.4314, 0.4353,  ..., 0.7922, 0.7922, 0.7922],\n",
       "         ...,\n",
       "         [0.7765, 0.7922, 0.8039,  ..., 0.8392, 0.8353, 0.8314],\n",
       "         [0.7882, 0.8078, 0.8078,  ..., 0.8353, 0.8314, 0.8275],\n",
       "         [0.8000, 0.8000, 0.8000,  ..., 0.8275, 0.8235, 0.8235]],\n",
       "\n",
       "        [[0.4706, 0.4745, 0.4824,  ..., 0.8235, 0.8235, 0.8235],\n",
       "         [0.4745, 0.4745, 0.4824,  ..., 0.8235, 0.8235, 0.8235],\n",
       "         [0.4745, 0.4784, 0.4824,  ..., 0.8275, 0.8275, 0.8275],\n",
       "         ...,\n",
       "         [0.7765, 0.7922, 0.8039,  ..., 0.8392, 0.8353, 0.8314],\n",
       "         [0.7882, 0.8078, 0.8078,  ..., 0.8353, 0.8314, 0.8275],\n",
       "         [0.8000, 0.8000, 0.8000,  ..., 0.8275, 0.8235, 0.8235]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best training performance is observed when input data values fall in the ranges [0, 1] or [-1, 1]\n",
    "\n",
    "batch = batch.float()\n",
    "batch /= 255.0\n",
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "May also want to compute the mean sand stdev of the input data and scale it\n",
    "so that the output has zero mean and unit stdev across each channel\n",
    "\n",
    "Torch provides functions for calculating these for tensors\n",
    "\"\"\"\n",
    "\n",
    "n_channels = batch.shape[1]\n",
    "for c in range(n_channels):\n",
    "    mean = torch.mean(batch[:, c])\n",
    "    std = torch.std(batch[:, c])\n",
    "    batch[:, c] = (batch[:, c] - mean) / std\n",
    "    \n",
    "# NOTE: it's good practice to compute the mean and stdev on all training data in \n",
    "# advance and then subtract nad divide by these fixed, precomputed quanities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D images\n",
    "\n",
    "In some domains, sequences of images are stacked along the head-to-foot axis. E.g. the slices in CT scans.\n",
    "\n",
    "By stacking individual 2D slices into a 3D tensor, we can built _volumetric data_ representing the 3D anatomy of a subject. Storing volumetric data is just like storing image data, except that an extra dimension, _depth_, comes after the standard channel dimension, resulting in a 5D tensor of shape `N x C x D x H x W`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DICOM (examining files): 1/99 files (1.0%7/99 files (7.1%15/99 files (15.2%18/99 files (18.2%26/99 files (26.3%35/99 files (35.4%44/99 files (44.4%50/99 files (50.5%62/99 files (62.6%66/99 files (66.7%79/99 files (79.8%88/99 files (88.9%92/99 files (92.9%99/99 files (100.0%)\n",
      "  Found 1 correct series.\n",
      "Reading DICOM (loading data): 9/99  (9.127/99  (27.344/99  (44.456/99  (56.679/99  (79.892/99  (92.999/99  (100.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99, 512, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the specialised format\n",
    "# Volumetric data can be downloaded from: https://github.com/deep-learning-with-pytorch/dlwpt-code/tree/master/data/p1ch4/volumetric-dicom/2-LUNG%203.0%20%20B70f-04083\n",
    "\n",
    "import imageio\n",
    "dir_path = \"../data/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083\"\n",
    "vol_arr = imageio.volread(dir_path, 'DICOM')\n",
    "vol_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.045</td>\n",
       "      <td>30.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.044</td>\n",
       "      <td>28.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "5            8.1              0.28         0.40             6.9      0.050   \n",
       "6            6.2              0.32         0.16             7.0      0.045   \n",
       "7            7.0              0.27         0.36            20.7      0.045   \n",
       "8            6.3              0.30         0.34             1.6      0.049   \n",
       "9            8.1              0.22         0.43             1.5      0.044   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "5                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "6                 30.0                 136.0   0.9949  3.18       0.47   \n",
       "7                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "8                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "9                 28.0                 129.0   0.9938  3.22       0.45   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  \n",
       "5     10.1        6  \n",
       "6      9.6        6  \n",
       "7      8.8        6  \n",
       "8      9.5        6  \n",
       "9     11.0        6  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As pandas is recommended, I used pandas instead of numpy (which is used in the book),\n",
    "# as I need to get comfortable with it.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "wine_path = \"../data/p1ch4/tabular-wine/winequality-white.csv\"\n",
    "wineq_df = pd.read_csv(wine_path, delimiter=\";\")\n",
    "wineq_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4898, 12),\n",
       " Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "        'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "        'pH', 'sulphates', 'alcohol', 'quality'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq_df.shape, wineq_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 12]), torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq = torch.from_numpy(wineq_df.values)\n",
    "wineq.shape, wineq.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.0000,  0.2700,  0.3600,  ...,  3.0000,  0.4500,  8.8000],\n",
       "         [ 6.3000,  0.3000,  0.3400,  ...,  3.3000,  0.4900,  9.5000],\n",
       "         [ 8.1000,  0.2800,  0.4000,  ...,  3.2600,  0.4400, 10.1000],\n",
       "         ...,\n",
       "         [ 6.5000,  0.2400,  0.1900,  ...,  2.9900,  0.4600,  9.4000],\n",
       "         [ 5.5000,  0.2900,  0.3000,  ...,  3.3400,  0.3800, 12.8000],\n",
       "         [ 6.0000,  0.2100,  0.3800,  ...,  3.2600,  0.3200, 11.8000]],\n",
       "        dtype=torch.float64),\n",
       " torch.Size([4898, 11]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Choices with the \"quality\" score:\n",
    "\n",
    "1. Treat as a continuous variable, keep it as a real number, perform a regression task\n",
    "    (by definition real number prediction)\n",
    "2. Treat it as a label and try to guess the label from the chemical analysis in a classification task\n",
    "\n",
    "Regardless, we separate this out so it's not an input which influences model training,\n",
    "as this is what we're trying to predict.\n",
    "\"\"\"\n",
    "\n",
    "data = wineq[:, :-1] # Selects all rows and all columns except the last\n",
    "data, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6., 6., 6.,  ..., 6., 7., 6.], dtype=torch.float64),\n",
       " torch.Size([4898]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = wineq[:, -1] # Selects all rows and the last column\n",
    "target, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6,  ..., 6, 7, 6])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert targets to integer vector of scores, which we then one hot encode (target.unsqueeze() below)\n",
    "# We could also just keep it like this\n",
    "\n",
    "target = wineq[:, -1].long()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now treat then targets as an integer vector of scores, or one-hot encode.\n",
    "# One-hot encoding has the advantage of not implying (and thus embedding) ordering or distance between targets\n",
    "\n",
    "target_onehot = torch.zeros(target.shape[0], 10)\n",
    "target_onehot.scatter_(1, target.unsqueeze(1), 1.0) # trailing underscore == in place operation\n",
    "target_onehot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpart1",
   "language": "python",
   "name": "venvpart1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
