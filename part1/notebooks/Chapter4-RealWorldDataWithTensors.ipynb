{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to represent different types of data with tensors. Types covered: images, tabular, time series, text.\n",
    "\n",
    "## Working with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 855, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "img_arr = imageio.imread(\"../data/p1ch4/image-dog/bobby.jpg\")\n",
    "img_arr.shape # H x W x C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1280, 855])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "img = torch.from_numpy(img_arr)\n",
    "out = img.permute(2, 0, 1) # Permute to fit C x H x W dimension ordering\n",
    "out.shape\n",
    "\n",
    "# Note: permute doesn't create a new image, but alters the size and stride information at the level\n",
    "# of the original tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To creat a dataset of multiple images to use as input for our neural networks, we store the images\n",
    "in a batch along the first dimension to obtain an N x C x H x W tensor.\n",
    "\"\"\"\n",
    "\n",
    "# An efficient way to create a batch is pre-allocation followed by loading from a directory\n",
    "\n",
    "batch_size = 3\n",
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '../data/p1ch4/image-cats/'\n",
    "filenames = [name for name in os.listdir(data_dir)\n",
    "             if os.path.splitext(name)[-1] == \".png\"] # Condition ensures images used are of a desired format.\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "\n",
    "    img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "    img_t = torch.from_numpy(img_arr)\n",
    "    img_t = img_t.permute(2, 0, 1)\n",
    "    img_t = img_t[:3]\n",
    "    batch[i] = img_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 90,  91,  93,  ..., 191, 191, 191],\n",
       "         [ 91,  91,  93,  ..., 191, 191, 191],\n",
       "         [ 91,  92,  93,  ..., 192, 192, 192],\n",
       "         ...,\n",
       "         [206, 210, 213,  ..., 220, 219, 218],\n",
       "         [209, 214, 214,  ..., 221, 220, 219],\n",
       "         [212, 212, 212,  ..., 219, 218, 218]],\n",
       "\n",
       "        [[108, 109, 111,  ..., 201, 201, 201],\n",
       "         [109, 109, 111,  ..., 201, 201, 201],\n",
       "         [109, 110, 111,  ..., 202, 202, 202],\n",
       "         ...,\n",
       "         [198, 202, 205,  ..., 214, 213, 212],\n",
       "         [201, 206, 206,  ..., 213, 212, 211],\n",
       "         [204, 204, 204,  ..., 211, 210, 210]],\n",
       "\n",
       "        [[120, 121, 123,  ..., 210, 210, 210],\n",
       "         [121, 121, 123,  ..., 210, 210, 210],\n",
       "         [121, 122, 123,  ..., 211, 211, 211],\n",
       "         ...,\n",
       "         [198, 202, 205,  ..., 214, 213, 212],\n",
       "         [201, 206, 206,  ..., 213, 212, 211],\n",
       "         [204, 204, 204,  ..., 211, 210, 210]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3529, 0.3569, 0.3647,  ..., 0.7490, 0.7490, 0.7490],\n",
       "         [0.3569, 0.3569, 0.3647,  ..., 0.7490, 0.7490, 0.7490],\n",
       "         [0.3569, 0.3608, 0.3647,  ..., 0.7529, 0.7529, 0.7529],\n",
       "         ...,\n",
       "         [0.8078, 0.8235, 0.8353,  ..., 0.8627, 0.8588, 0.8549],\n",
       "         [0.8196, 0.8392, 0.8392,  ..., 0.8667, 0.8627, 0.8588],\n",
       "         [0.8314, 0.8314, 0.8314,  ..., 0.8588, 0.8549, 0.8549]],\n",
       "\n",
       "        [[0.4235, 0.4275, 0.4353,  ..., 0.7882, 0.7882, 0.7882],\n",
       "         [0.4275, 0.4275, 0.4353,  ..., 0.7882, 0.7882, 0.7882],\n",
       "         [0.4275, 0.4314, 0.4353,  ..., 0.7922, 0.7922, 0.7922],\n",
       "         ...,\n",
       "         [0.7765, 0.7922, 0.8039,  ..., 0.8392, 0.8353, 0.8314],\n",
       "         [0.7882, 0.8078, 0.8078,  ..., 0.8353, 0.8314, 0.8275],\n",
       "         [0.8000, 0.8000, 0.8000,  ..., 0.8275, 0.8235, 0.8235]],\n",
       "\n",
       "        [[0.4706, 0.4745, 0.4824,  ..., 0.8235, 0.8235, 0.8235],\n",
       "         [0.4745, 0.4745, 0.4824,  ..., 0.8235, 0.8235, 0.8235],\n",
       "         [0.4745, 0.4784, 0.4824,  ..., 0.8275, 0.8275, 0.8275],\n",
       "         ...,\n",
       "         [0.7765, 0.7922, 0.8039,  ..., 0.8392, 0.8353, 0.8314],\n",
       "         [0.7882, 0.8078, 0.8078,  ..., 0.8353, 0.8314, 0.8275],\n",
       "         [0.8000, 0.8000, 0.8000,  ..., 0.8275, 0.8235, 0.8235]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best training performance is observed when input data values fall in the ranges [0, 1] or [-1, 1]\n",
    "\n",
    "batch = batch.float()\n",
    "batch /= 255.0\n",
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "May also want to compute the mean sand stdev of the input data and scale it\n",
    "so that tehe output has zero mean and unit stdev across each channel\n",
    "\n",
    "Torch provides functions for calculating these for tensors\n",
    "\"\"\"\n",
    "\n",
    "n_channels = batch.shape[1]\n",
    "for c in range(n_channels):\n",
    "    mean = torch.mean(batch[:, c])\n",
    "    std = torch.std(batch[:, c])\n",
    "    batch[:, c] = (batch[:, c] - mean) / std\n",
    "    \n",
    "# NOTE: it's good practice to compute the mean and stdev on all training data in \n",
    "# advance and then subtract nad divide by these fixed, precomputed quanities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D images\n",
    "\n",
    "In some domains, sequences of images are stacked along the head-to-foot axis. E.g. the slices in CT scans.\n",
    "\n",
    "By stacking individual 2D slices into a 3D tensor, we can built _volumetric data_ representing the 3D anatomy of a subject. Storing volumetric data is just like storing image data, except that an extra dimension, _depth_, comes after the standard channel dimension, resulting in a 5D tensor of shape `N x C x D x H x W`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DICOM (examining files): 1/99 files (1.0%99/99 files (100.0%)\n",
      "  Found 1 correct series.\n",
      "Reading DICOM (loading data): 85/99  (85.999/99  (100.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99, 512, 512)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the specialised format\n",
    "# Volumetric data can be downloaded from: https://github.com/deep-learning-with-pytorch/dlwpt-code/tree/master/data/p1ch4/volumetric-dicom/2-LUNG%203.0%20%20B70f-04083\n",
    "\n",
    "import imageio\n",
    "dir_path = \"../data/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083\"\n",
    "vol_arr = imageio.volread(dir_path, 'DICOM')\n",
    "vol_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing tabular data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpart1",
   "language": "python",
   "name": "venvpart1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
