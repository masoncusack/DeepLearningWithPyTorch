{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note that the PyTorch nn module provides 3D convolutions for volumes or videos.\n",
    "For now though we'll stick to 2D examples.\n",
    "\n",
    "(3D convolution is used for operating on CT scan data in Part 2)\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(3, 16, kernel_size=3) # 3-channel, 3x3 kernel with 16 out channels\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinit cifar2\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "data_path = \"../data/cifar-10/\"\n",
    "\n",
    "\n",
    "class_names = [\n",
    "    \"Airplane\",\n",
    "    \"Automobile\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\"\n",
    "]\n",
    "\n",
    "# Add to dataset transforms\n",
    "transformed_cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "transformed_cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    ")\n",
    "   \n",
    "\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "cifar2 = [\n",
    "    (img, label_map[label])\n",
    "    for img, label in transformed_cifar10\n",
    "    if label in [0, 2]\n",
    "]\n",
    "\n",
    "\n",
    "cifar2_val = [\n",
    "    (img, label_map[label])\n",
    "    for img, label in transformed_cifar10_val\n",
    "    if label in [0, 2]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolution i/o example\n",
    "\n",
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0)) # requires dimensions B x C x H x W\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bird after convolution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Bird after convolution:\")\n",
    "\n",
    "plt.imshow(output[0, 0].detach(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downsampling is common in multi-layer convolution, to ensure content \n",
    "initially larger than our kernels can be captured. This can be done using pooling\n",
    "or specially strided convolutions. We will focus on maxpooling, which has until now\n",
    "proved most effective.\n",
    "\"\"\"\n",
    "\n",
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Tanh()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Tanh()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Flatten()\n",
       "  (7): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (8): Tanh()\n",
       "  (9): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A convolutional model (basic CNN)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # End with fully connected layers\n",
    "    # Flatten required to transform the 8x8 image up to this point into 512x1 vector\n",
    "    # We can (and used to have to) forego this by subclassing nn.Module as below,\n",
    "    # which means abandoning the use of nn.Sequential\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(8 * 8 * 8, 32),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(32, 2)\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick parameter count\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative approach to Flatten: subclassing nn.Module.\n",
    "\n",
    "This is also often required for other operations which the provided modules don't supply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    The Net class is equivalent to the nn.Sequential model\n",
    "    we built earlier in terms of submodules; but by writing\n",
    "    the forward function explicitly, we can manipulate the\n",
    "    output of self.pool3 directly and call view on it to turn\n",
    "    it into a B × N vector (see out.view in the forward() function.\n",
    "    This is now allowed in an nn.Sequential model, for reasons.\n",
    "    \n",
    "    https://github.com/pytorch/pytorch/issues/2486\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functional API (torch.nn.functional) provides counterparts for nn.Modules\n",
    "# which have no internal state (e.g. parameters). This means modules which do not\n",
    "# require backprop can be placed in the forward() function\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A version of the prior Net model utilising the Functional API.\n",
    "    \n",
    "    It doesn't matter if we don't use it, but it can make the boilerplate\n",
    "    more concise as below.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        # tanh as a general-purpose scientific function remains in the torch namespaces\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1287, -0.0501]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does it run?\n",
    "\n",
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop definition\n",
    "\n",
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(\"{} Epoch {}, Training loss {}\".format(\n",
    "                datetime.datetime.now(),\n",
    "                epoch,\n",
    "                loss_train / len(train_loader)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-28 19:28:15.799093 Epoch 1, Training loss 0.5612184840023138\n",
      "2020-12-28 19:28:24.063061 Epoch 10, Training loss 0.3264619844734289\n",
      "2020-12-28 19:28:33.575720 Epoch 20, Training loss 0.2881455597034685\n",
      "2020-12-28 19:28:42.629494 Epoch 30, Training loss 0.2683769628690307\n",
      "2020-12-28 19:28:51.676197 Epoch 40, Training loss 0.2463007071025812\n",
      "2020-12-28 19:29:00.909399 Epoch 50, Training loss 0.23164321173718022\n",
      "2020-12-28 19:29:10.005691 Epoch 60, Training loss 0.21244540976680767\n",
      "2020-12-28 19:29:19.302773 Epoch 70, Training loss 0.19450994841991717\n",
      "2020-12-28 19:29:28.445251 Epoch 80, Training loss 0.18192931019054476\n",
      "2020-12-28 19:29:37.636411 Epoch 90, Training loss 0.1691721574800789\n",
      "2020-12-28 19:29:46.854347 Epoch 100, Training loss 0.15641561186142788\n"
     ]
    }
   ],
   "source": [
    "# Dataloader init and run\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.94\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Measuring accuracy on validation set\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading the model\n",
    "\n",
    "torch.save(model.state_dict(), data_path + \"birds_vs_airplanes_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(data_path + \"birds_vs_airplanes_v1.pt\"))\n",
    "\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "# Training on GPU requires specification of which device you're using,\n",
    "# And casting the model to it.\n",
    "\n",
    "# NOTE: CUDA installation was messed up when this was written. TODO: run this on a working\n",
    "# GPU machine at some point.\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "    \n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = model(imgs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss.item()\n",
    "        \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-28 19:43:51.924206 Epoch 100, Training loss 0.004487212296504124\n",
      "2020-12-28 19:43:51.930103 Epoch 100, Training loss 0.008951313936026993\n",
      "2020-12-28 19:43:51.935729 Epoch 100, Training loss 0.01344850450564342\n",
      "2020-12-28 19:43:51.941307 Epoch 100, Training loss 0.017926076794885525\n",
      "2020-12-28 19:43:51.947223 Epoch 100, Training loss 0.022383221395456107\n",
      "2020-12-28 19:43:51.952843 Epoch 100, Training loss 0.026810116828626888\n",
      "2020-12-28 19:43:51.959334 Epoch 100, Training loss 0.031256881489115915\n",
      "2020-12-28 19:43:51.965012 Epoch 100, Training loss 0.03568487941839133\n",
      "2020-12-28 19:43:51.970676 Epoch 100, Training loss 0.0400754877716113\n",
      "2020-12-28 19:43:51.976886 Epoch 100, Training loss 0.044448992249312674\n",
      "2020-12-28 19:43:51.982446 Epoch 100, Training loss 0.04879681927383326\n",
      "2020-12-28 19:43:51.988098 Epoch 100, Training loss 0.053110405517991184\n",
      "2020-12-28 19:43:51.995993 Epoch 100, Training loss 0.05744798889585361\n",
      "2020-12-28 19:43:52.001740 Epoch 100, Training loss 0.06175317657980949\n",
      "2020-12-28 19:43:52.008085 Epoch 100, Training loss 0.06600582789463602\n",
      "2020-12-28 19:43:52.013905 Epoch 100, Training loss 0.07025435509955048\n",
      "2020-12-28 19:43:52.019771 Epoch 100, Training loss 0.07450379459721268\n",
      "2020-12-28 19:43:52.025642 Epoch 100, Training loss 0.07869642555334005\n",
      "2020-12-28 19:43:52.031291 Epoch 100, Training loss 0.0829409899984955\n",
      "2020-12-28 19:43:52.038090 Epoch 100, Training loss 0.08714798795189826\n",
      "2020-12-28 19:43:52.045011 Epoch 100, Training loss 0.09138912494015541\n",
      "2020-12-28 19:43:52.051250 Epoch 100, Training loss 0.09551301275848582\n",
      "2020-12-28 19:43:52.059294 Epoch 100, Training loss 0.09958120895798798\n",
      "2020-12-28 19:43:52.065366 Epoch 100, Training loss 0.10367743756361068\n",
      "2020-12-28 19:43:52.072799 Epoch 100, Training loss 0.1077251517848604\n",
      "2020-12-28 19:43:52.078775 Epoch 100, Training loss 0.11189046597025197\n",
      "2020-12-28 19:43:52.085395 Epoch 100, Training loss 0.11593173757480209\n",
      "2020-12-28 19:43:52.091728 Epoch 100, Training loss 0.11995782631977349\n",
      "2020-12-28 19:43:52.097467 Epoch 100, Training loss 0.12412775436024757\n",
      "2020-12-28 19:43:52.104136 Epoch 100, Training loss 0.1283156222598568\n",
      "2020-12-28 19:43:52.109960 Epoch 100, Training loss 0.13234346421660892\n",
      "2020-12-28 19:43:52.116283 Epoch 100, Training loss 0.13631245664730193\n",
      "2020-12-28 19:43:52.122403 Epoch 100, Training loss 0.14023492670362922\n",
      "2020-12-28 19:43:52.130322 Epoch 100, Training loss 0.14395660921266884\n",
      "2020-12-28 19:43:52.136462 Epoch 100, Training loss 0.1481276226651137\n",
      "2020-12-28 19:43:52.142223 Epoch 100, Training loss 0.1520927981206566\n",
      "2020-12-28 19:43:52.149121 Epoch 100, Training loss 0.15613076603336698\n",
      "2020-12-28 19:43:52.154868 Epoch 100, Training loss 0.16020080542108814\n",
      "2020-12-28 19:43:52.161259 Epoch 100, Training loss 0.16402321142755497\n",
      "2020-12-28 19:43:52.167200 Epoch 100, Training loss 0.16791330894846826\n",
      "2020-12-28 19:43:52.172998 Epoch 100, Training loss 0.17166712481504792\n",
      "2020-12-28 19:43:52.179751 Epoch 100, Training loss 0.17540949355265137\n",
      "2020-12-28 19:43:52.185680 Epoch 100, Training loss 0.17915647386745281\n",
      "2020-12-28 19:43:52.191730 Epoch 100, Training loss 0.18290390140691382\n",
      "2020-12-28 19:43:52.199494 Epoch 100, Training loss 0.18664598502930563\n",
      "2020-12-28 19:43:52.205934 Epoch 100, Training loss 0.19065463580902975\n",
      "2020-12-28 19:43:52.214660 Epoch 100, Training loss 0.1941727703543985\n",
      "2020-12-28 19:43:52.220502 Epoch 100, Training loss 0.19748961621788658\n",
      "2020-12-28 19:43:52.228687 Epoch 100, Training loss 0.20126271020075318\n",
      "2020-12-28 19:43:52.235640 Epoch 100, Training loss 0.2046620895148842\n",
      "2020-12-28 19:43:52.241703 Epoch 100, Training loss 0.20824553746326713\n",
      "2020-12-28 19:43:52.247441 Epoch 100, Training loss 0.21185464046563313\n",
      "2020-12-28 19:43:52.253269 Epoch 100, Training loss 0.21540861600523542\n",
      "2020-12-28 19:43:52.263561 Epoch 100, Training loss 0.21867802150689872\n",
      "2020-12-28 19:43:52.269405 Epoch 100, Training loss 0.2220511527577783\n",
      "2020-12-28 19:43:52.275310 Epoch 100, Training loss 0.22563168956975269\n",
      "2020-12-28 19:43:52.281126 Epoch 100, Training loss 0.22936700521760686\n",
      "2020-12-28 19:43:52.286958 Epoch 100, Training loss 0.23318475029271118\n",
      "2020-12-28 19:43:52.292720 Epoch 100, Training loss 0.2369750244602276\n",
      "2020-12-28 19:43:52.298410 Epoch 100, Training loss 0.2405568828248674\n",
      "2020-12-28 19:43:52.304285 Epoch 100, Training loss 0.2440263716278562\n",
      "2020-12-28 19:43:52.310014 Epoch 100, Training loss 0.24769907772161398\n",
      "2020-12-28 19:43:52.315696 Epoch 100, Training loss 0.25086574588611626\n",
      "2020-12-28 19:43:52.322639 Epoch 100, Training loss 0.2545033972354452\n",
      "2020-12-28 19:43:52.328434 Epoch 100, Training loss 0.258824462154109\n",
      "2020-12-28 19:43:52.337336 Epoch 100, Training loss 0.2624636036195573\n",
      "2020-12-28 19:43:52.343226 Epoch 100, Training loss 0.26620574874483094\n",
      "2020-12-28 19:43:52.349280 Epoch 100, Training loss 0.26962877363915655\n",
      "2020-12-28 19:43:52.355122 Epoch 100, Training loss 0.2732174182014101\n",
      "2020-12-28 19:43:52.360850 Epoch 100, Training loss 0.2768918600431673\n",
      "2020-12-28 19:43:52.368298 Epoch 100, Training loss 0.28037850890949273\n",
      "2020-12-28 19:43:52.373955 Epoch 100, Training loss 0.28359680589596936\n",
      "2020-12-28 19:43:52.379945 Epoch 100, Training loss 0.287007403411683\n",
      "2020-12-28 19:43:52.385674 Epoch 100, Training loss 0.2901934899721935\n",
      "2020-12-28 19:43:52.391414 Epoch 100, Training loss 0.2933220863342285\n",
      "2020-12-28 19:43:52.397331 Epoch 100, Training loss 0.29642942756604235\n",
      "2020-12-28 19:43:52.402971 Epoch 100, Training loss 0.299702101072688\n",
      "2020-12-28 19:43:52.408835 Epoch 100, Training loss 0.3031634802271606\n",
      "2020-12-28 19:43:52.416347 Epoch 100, Training loss 0.3070322472578401\n",
      "2020-12-28 19:43:52.424391 Epoch 100, Training loss 0.31053901743736995\n",
      "2020-12-28 19:43:52.431482 Epoch 100, Training loss 0.31429666622429137\n",
      "2020-12-28 19:43:52.437344 Epoch 100, Training loss 0.31788501769873745\n",
      "2020-12-28 19:43:52.443996 Epoch 100, Training loss 0.32114270879964163\n",
      "2020-12-28 19:43:52.454742 Epoch 100, Training loss 0.32504212628504275\n",
      "2020-12-28 19:43:52.461362 Epoch 100, Training loss 0.3287932717116775\n",
      "2020-12-28 19:43:52.469228 Epoch 100, Training loss 0.33171105878368307\n",
      "2020-12-28 19:43:52.475449 Epoch 100, Training loss 0.33507113282088263\n",
      "2020-12-28 19:43:52.481337 Epoch 100, Training loss 0.3383824103956769\n",
      "2020-12-28 19:43:52.487037 Epoch 100, Training loss 0.3413334781197226\n",
      "2020-12-28 19:43:52.492982 Epoch 100, Training loss 0.34441354244377964\n",
      "2020-12-28 19:43:52.498637 Epoch 100, Training loss 0.348917839253784\n",
      "2020-12-28 19:43:52.504967 Epoch 100, Training loss 0.3515314850837562\n",
      "2020-12-28 19:43:52.510966 Epoch 100, Training loss 0.3547246513093353\n",
      "2020-12-28 19:43:52.516621 Epoch 100, Training loss 0.35789101841343435\n",
      "2020-12-28 19:43:52.522487 Epoch 100, Training loss 0.36107330944887395\n",
      "2020-12-28 19:43:52.528208 Epoch 100, Training loss 0.3650010938097717\n",
      "2020-12-28 19:43:52.533890 Epoch 100, Training loss 0.3674823573440503\n",
      "2020-12-28 19:43:52.544900 Epoch 100, Training loss 0.3708879837564602\n",
      "2020-12-28 19:43:52.552610 Epoch 100, Training loss 0.37423629403873615\n",
      "2020-12-28 19:43:52.561634 Epoch 100, Training loss 0.37793260319217753\n",
      "2020-12-28 19:43:52.568630 Epoch 100, Training loss 0.38145179657419775\n",
      "2020-12-28 19:43:52.576625 Epoch 100, Training loss 0.385058291778443\n",
      "2020-12-28 19:43:52.586415 Epoch 100, Training loss 0.38845731991871146\n",
      "2020-12-28 19:43:52.592218 Epoch 100, Training loss 0.3919217708004508\n",
      "2020-12-28 19:43:52.599416 Epoch 100, Training loss 0.3953053537447741\n",
      "2020-12-28 19:43:52.608889 Epoch 100, Training loss 0.3990800661645877\n",
      "2020-12-28 19:43:52.615541 Epoch 100, Training loss 0.40254423307005766\n",
      "2020-12-28 19:43:52.621356 Epoch 100, Training loss 0.4056956718681724\n",
      "2020-12-28 19:43:52.627016 Epoch 100, Training loss 0.4089481678737956\n",
      "2020-12-28 19:43:52.633142 Epoch 100, Training loss 0.4120901272555066\n",
      "2020-12-28 19:43:52.638871 Epoch 100, Training loss 0.4153040442497108\n",
      "2020-12-28 19:43:52.646123 Epoch 100, Training loss 0.4183662444542927\n",
      "2020-12-28 19:43:52.652238 Epoch 100, Training loss 0.4220320185658279\n",
      "2020-12-28 19:43:52.657909 Epoch 100, Training loss 0.4251099428173843\n",
      "2020-12-28 19:43:52.664429 Epoch 100, Training loss 0.42879723458533076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-28 19:43:52.672284 Epoch 100, Training loss 0.43313802218740916\n",
      "2020-12-28 19:43:52.678264 Epoch 100, Training loss 0.4367838856900573\n",
      "2020-12-28 19:43:52.683970 Epoch 100, Training loss 0.43959505846545954\n",
      "2020-12-28 19:43:52.689786 Epoch 100, Training loss 0.44243250398119544\n",
      "2020-12-28 19:43:52.695634 Epoch 100, Training loss 0.44570846219730986\n",
      "2020-12-28 19:43:52.701361 Epoch 100, Training loss 0.4496597907725413\n",
      "2020-12-28 19:43:52.707518 Epoch 100, Training loss 0.45261534080383886\n",
      "2020-12-28 19:43:52.715238 Epoch 100, Training loss 0.45613659377310684\n",
      "2020-12-28 19:43:52.721214 Epoch 100, Training loss 0.45887544333555136\n",
      "2020-12-28 19:43:52.728069 Epoch 100, Training loss 0.46198430467563073\n",
      "2020-12-28 19:43:52.733699 Epoch 100, Training loss 0.46571903718504937\n",
      "2020-12-28 19:43:52.743787 Epoch 100, Training loss 0.4690309594011611\n",
      "2020-12-28 19:43:52.757943 Epoch 100, Training loss 0.4730649167185376\n",
      "2020-12-28 19:43:52.768715 Epoch 100, Training loss 0.4769010036993938\n",
      "2020-12-28 19:43:52.781385 Epoch 100, Training loss 0.47976359762963217\n",
      "2020-12-28 19:43:52.791423 Epoch 100, Training loss 0.4831806461142886\n",
      "2020-12-28 19:43:52.807752 Epoch 100, Training loss 0.48607786483825394\n",
      "2020-12-28 19:43:52.813655 Epoch 100, Training loss 0.4891337816882285\n",
      "2020-12-28 19:43:52.819957 Epoch 100, Training loss 0.49215862041066405\n",
      "2020-12-28 19:43:52.831344 Epoch 100, Training loss 0.49547381404858487\n",
      "2020-12-28 19:43:52.837374 Epoch 100, Training loss 0.4983617147062994\n",
      "2020-12-28 19:43:52.842947 Epoch 100, Training loss 0.5009294659089131\n",
      "2020-12-28 19:43:52.848941 Epoch 100, Training loss 0.5041930662218932\n",
      "2020-12-28 19:43:52.854972 Epoch 100, Training loss 0.5075563824480507\n",
      "2020-12-28 19:43:52.860436 Epoch 100, Training loss 0.5104687274641292\n",
      "2020-12-28 19:43:52.867209 Epoch 100, Training loss 0.5137842335518757\n",
      "2020-12-28 19:43:52.875022 Epoch 100, Training loss 0.5169956558828901\n",
      "2020-12-28 19:43:52.880974 Epoch 100, Training loss 0.5201554717911276\n",
      "2020-12-28 19:43:52.886625 Epoch 100, Training loss 0.5237396919423607\n",
      "2020-12-28 19:43:52.892090 Epoch 100, Training loss 0.5272507771944545\n",
      "2020-12-28 19:43:52.897860 Epoch 100, Training loss 0.5306264357582019\n",
      "2020-12-28 19:43:52.903529 Epoch 100, Training loss 0.534173852129347\n",
      "2020-12-28 19:43:52.911270 Epoch 100, Training loss 0.5372802017221026\n",
      "2020-12-28 19:43:52.916927 Epoch 100, Training loss 0.5406249364849868\n",
      "2020-12-28 19:43:52.922570 Epoch 100, Training loss 0.5440912085353948\n",
      "2020-12-28 19:43:52.928210 Epoch 100, Training loss 0.5475224824088394\n",
      "2020-12-28 19:43:52.933668 Epoch 100, Training loss 0.5509224733349624\n",
      "2020-12-28 19:43:52.939124 Epoch 100, Training loss 0.5545194382500497\n",
      "2020-12-28 19:43:52.944728 Epoch 100, Training loss 0.5576930456100755\n",
      "2020-12-28 19:43:52.950152 Epoch 100, Training loss 0.5606226740749019\n",
      "2020-12-28 19:43:52.955734 Epoch 100, Training loss 0.5635508545644724\n",
      "2020-12-28 19:43:52.962718 Epoch 100, Training loss 0.5668348073959351\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    cifar2,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We have the option of casting a model trained on a certain device to another device\n",
    "before saving its state dict, but it's more concise (particularly if we're not sure\n",
    "what kind of device we'll next use it on), if we pass the current device used as map_location\n",
    "when later loading a saved state_dict.\n",
    "\"\"\"\n",
    "\n",
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(\n",
    "    torch.load(\n",
    "        data_path + \"birds_vs_airplanes_v1.pt\",\n",
    "        map_location=device # Here\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpart1",
   "language": "python",
   "name": "venvpart1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
