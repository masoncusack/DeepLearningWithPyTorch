{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note that the PyTorch nn module provides 3D convolutions for volumes or videos.\n",
    "For now though we'll stick to 2D examples.\n",
    "\n",
    "(3D convolution is used for operating on CT scan data in Part 2)\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(3, 16, kernel_size=3) # 3-channel, 3x3 kernel with 16 out channels\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinit cifar2\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "data_path = \"../data/cifar-10/\"\n",
    "\n",
    "\n",
    "class_names = [\n",
    "    \"Airplane\",\n",
    "    \"Automobile\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\"\n",
    "]\n",
    "\n",
    "# Add to dataset transforms\n",
    "transformed_cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "transformed_cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    ")\n",
    "   \n",
    "\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "cifar2 = [\n",
    "    (img, label_map[label])\n",
    "    for img, label in transformed_cifar10\n",
    "    if label in [0, 2]\n",
    "]\n",
    "\n",
    "\n",
    "cifar2_val = [\n",
    "    (img, label_map[label])\n",
    "    for img, label in transformed_cifar10_val\n",
    "    if label in [0, 2]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolution i/o example\n",
    "\n",
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0)) # requires dimensions B x C x H x W\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bird after convolution:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWQklEQVR4nO2dW4hd53mGn0+25Mij0WGkSB5LtkY1CiEk1CmDKTg0KSHBNQEnNya+CC6EKhcxxJCLhvQivjQlB3JRAkpt4pQ0B0hMDDFtUhMwuTGWjWP50NqpYx1Go5NlnXyINKOvF7NVJur87z9ae2bvSf/3gWFm1jdr/9/693pn7b3f9X1/ZCbGmP//rBp2AsaYwWCxG9MIFrsxjWCxG9MIFrsxjWCxG9MI1/azc0TcAXwbuAb458x8UP39yMhIjo2NdRrr0qVLxdiqVeX/WddeWz5EFavFlWV54cKFYkwdB+hjUbF+UI97zTXXFGM12zYiirHVq1d3ymd2dlaO2XXu1ePWjlPtq2Jd81H7nj9/nnfffXfBie8s9oi4Bvgn4BPAYeDpiHgsM18q7TM2Nsb9999fejw53ttvv12MXX/99cWY+ueybds2OeamTZuKsZmZmWLs8OHDxdhbb70lx7zuuuuKsZGREblvidrJun79+mJsdHS0GPvDH/4gH3ft2rXF2Pj4eKf93nzzTTnmwYMHizE19+fOnSvGLl68KMdU+54+fboYU+e02g/Kx/KLX/yiuE8/l4rbgN9l5muZeQH4EXBXH49njFlG+hH7duDQvN8P97YZY1Ygy/4BXUTsiYh9EbGv9hLWGLN89CP2KeCmeb/v6G37IzJzb2ZOZuZk1/ecxpj+6UfsTwO7I2JXRKwBPgs8tjRpGWOWms6fxmfmTETcB/w7c9bbw5n5otpnZmaGN954Y8GYsnigbkWUUJ+k1j7xVJ9Eb9iwoRhTx1I7TvUpv3obtGbNmmJM2VzQ3SJTn5qDttCURaYcCTU/AEeOHCnGTp061SmfmlOkLDSV77vvvtspHyg7ISqXvnz2zHwceLyfxzDGDAbfQWdMI1jsxjSCxW5MI1jsxjSCxW5MI1jsxjRCX9bb1RIRRQ9VVV6B9qfPnj1bjJV8/cWgSlzf8573FGPKn6953jUfvsS6des6xUD72upYaiXC77zzTjGmKsWUx3zixAk5pqp6O3r0aDGmjkVV6IG+l6Orl37+/Hk5ZumcV/ej+MpuTCNY7MY0gsVuTCNY7MY0gsVuTCNY7MY0wkCtt8ws2hS1clNlbyg7S5Un1poXqkaWKta1TLW2r7InVT61rrSqqYhqVlmzNZV9pOxAZT+qMlXQz6k6h5SVWrMYVeNNFVMNJ5VlB+XzRD1fvrIb0wgWuzGNYLEb0wgWuzGNYLEb0wgWuzGNMFDrbWZmhpMnTy4Yq9lDqmumsp3Uem21CjP1uKqbq7JUaijrRFlHXWOgLTJlFdY6/qpjUZZoPx2BlWXVtQusqpYDXb3W1V6rddEt2YFqXn1lN6YRLHZjGsFiN6YRLHZjGsFiN6YRLHZjGqEv6y0iXgfOAbPATGZOqr+fnZ0tVkqppoegq6SUhaGaHtYqi5QdqBoxKmoL9ik7UFVQqaab/SwmqZ4XZU3W9lU22KFDh4qxp556So6pbLKNGzcWY+pcqJ0nan67LkiqqvCgfG6qXJbCZ//rzFzYPDfGrBj8Mt6YRuhX7An8MiKeiYg9S5GQMWZ56Pdl/EcycyoitgK/ioj/zMwn5/9B75/AHqi/xzPGLB99Xdkzc6r3/TjwKHDbAn+zNzMnM3Oy9iGcMWb56Cz2iBiJiNHLPwOfBF5YqsSMMUtLPy/jtwGP9qpsrgX+NTP/bUmyMsYsOZ3FnpmvAX9+NftcunSp6DPXOnieOXOmGFP+syqX3Lx5sxxTdXOVfqY4llp3WeU/q/JFVUqpynFBlwGvXbu2GKu9LVNdf9X9BgcOHCjG9u/fL8dU3WUnJiaKMbV445YtW+SYyktX93mo+zhqJd+1+IL7XPUexpg/SSx2YxrBYjemESx2YxrBYjemESx2YxphoN1lZ2dnixaasmmgm9UAuhupsvMADh48WIwpW05ZNbVyU2VJqTlSJbeqtBO0hda12y3o50zZk+fOnSvGaufJ+973vmLs/e9/fzGmns+axajOI1Vyq45TzfticloIX9mNaQSL3ZhGsNiNaQSL3ZhGsNiNaQSL3ZhGGKj1dunSpWLVV61bq6q+UjaOsrJqnV5VBZqqXlOPu3XrVjnmyMhIMTY2NlaM3XjjjcVYrepN2YGq82ytgk91SFUWmlposmYjKuvtQx/6UDGm5r3UEfkyXSvbFLUq0NL8eWFHY4zFbkwrWOzGNILFbkwjWOzGNILFbkwjDNR6U9QqqJSloGKqeqhWgabiys6qVSwplL2mqumUzVU7zq6NLFXVFmjbTi2WqCrFagse3nLLLcXYzp07izFlkalFM0FXzO3atasYUzZszRYu5assO1/ZjWkEi92YRrDYjWkEi92YRrDYjWkEi92YRrDYjWmEqs8eEQ8DnwKOZ+YHe9vGgB8DE8DrwN2ZWV5Rr0dmFv10VdYIugRW+ciqTFUtyAfaS1fdPVW55Lp16+SY119//ZLnU7uHQfnIqrzz1KlT8nHVfQHKo1f51EpcVSm08v2Vz658dND3Rtx8883FmDrn1WKlUPbTH3300eI+i7myfw+444ptXwGeyMzdwBO9340xK5iq2DPzSeDKf+F3AY/0fn4E+PTSpmWMWWq63i67LTOnez8fBbaV/jAi9gB7oN7g3xizfPT9AV3O3QhevBk8M/dm5mRmTtZa7Rhjlo+uYj8WEeMAve/Hly4lY8xy0FXsjwH39n6+F/j50qRjjFkuFmO9/RD4GLAlIg4DXwMeBH4SEZ8HDgB3L2awVatWFUsUlWUC3UtKlfVWK0VVVo163E2bNhVj27dvl2NOTEwUY8rSU9ZRzdZUVtebb5Yd1enp6WIMdPdZNbfK6qrZYAplQSrLs2b3qedbWa1qDmrWWwl1HFWxZ+Y9hdDHO2VjjBkKvoPOmEaw2I1pBIvdmEaw2I1pBIvdmEYY6C1tEVGszlJVW5f37RJTVkStg6eyRpQNNj4+Xoy9973vlWN2rXpT1lvN1lQoe1LNAehqRGXLbdtWvPuaDRs2dB5TVTkqi7GGstfUmGr+ui7GqY7fV3ZjGsFiN6YRLHZjGsFiN6YRLHZjGsFiN6YRBmq9rVq1qmgD1ewh1eWma/WQ2g90k0tltyjbpLYwobIRFcoiqzXWVLbnjh07ijFV+Qd68cbjx7u1QKhZUupY1DmkrLfac3LixIlOsRtuuKEYq50npUYw6jzwld2YRrDYjWkEi92YRrDYjWkEi92YRrDYjWkEi92YRhh4iWvJP6yVuKoSTuWf9rMwhfLSVUzlU/OmVdltbd+u+6myUbVoYe0+haNHjxZjyg8+ffp0p3wA3n777WJMlX+q86Sf8mvV2Vd5+7VS3pIe1HPtK7sxjWCxG9MIFrsxjWCxG9MIFrsxjWCxG9MIi1nY8WHgU8DxzPxgb9sDwN8Bl+v3vpqZj1cHu/baonWiyklBd+JU5bFq0cJ33nlHjqkeV5Vanjt3rhirlUuq7rJqX3UstQUslV2jjrNmSSkbUZXdnjp1qhirWVKbN28uxrrat2pBSNDzq84/NQfqPADYsmXLgtvVMS7myv494I4Ftn8rM2/tfVWFbowZLlWxZ+aTQPlfrTHmT4J+3rPfFxHPR8TDEVFeoNoYsyLoKvbvALcAtwLTwDdKfxgReyJiX0TsU22KjDHLSyexZ+axzJzNzEvAd4HbxN/uzczJzJys9dUyxiwfncQeEfMXM/sM8MLSpGOMWS4WY739EPgYsCUiDgNfAz4WEbcCCbwOfGExg61bt47bb7+9GFOoBRFVxdKRI0eKsampKTmmsk1U11pV7aVsOYCNGzcWY6pqS1V71SzGrrZdraJQPa7qLvvGG28UYzVLSs2fmiP1fNZQ9qR666oq+NRxQH2B0IWoij0z71lg80NXPZIxZqj4DjpjGsFiN6YRLHZjGsFiN6YRLHZjGsFiN6YRBtpddnR0lI9+9KMLJ1LxbNevX99pTHXXXs3zVj686hqqvOna/QRnzpwpxmormJaolWiqY1GetyphBd19dnp6uhhTfnitFFrNn/LSVafXt956S46pSo9VGbB63NrdpqVzQT0nvrIb0wgWuzGNYLEb0wgWuzGNYLEb0wgWuzGNMFDrbc2aNUxMTCwYU3YLVLpmiljXRfcATpw4UYwtV9dQZa9t2lTu/qWsy5r1puwqVa5bs6TUc6qsLmVX1RapVHOv9lXnST+lvAo1P6o0G8rHouxFX9mNaQSL3ZhGsNiNaQSL3ZhGsNiNaQSL3ZhGGKj1FhFFG6O24KGyj5Q1oqwI1eG0Flc2juooeuzYMTmmshFVR1G18KWyCQFOnjxZjKlFFmsdWZW1pKqzduzYUYyp+QFtval9b7zxxs5jKotWdQRWlW21RTNLc6vOS1/ZjWkEi92YRrDYjWkEi92YRrDYjWkEi92YRljMwo43Ad8HtjG3kOPezPx2RIwBPwYmmFvc8e7MLJcy9ShZI7XKLGVnKbtBVWbVrCNl46hKJ2VXqQaOoJtVKotMNbJUcwfamlOVgaqhJGh7bcOGDcWYshiVRQa6Mamah61btxZj4+PjxRjoRRhVU1NV4VhbuLF0XisLezFX9hngy5n5AeAvgS9GxAeArwBPZOZu4Ine78aYFUpV7Jk5nZnP9n4+B7wMbAfuAh7p/dkjwKeXKUdjzBJwVe/ZI2IC+DDwFLAtMy83/z7K3Mt8Y8wKZdFij4h1wE+B+zPzj97k5dwbzQXfbEbEnojYFxH71G2FxpjlZVFij4jVzAn9B5n5s97mYxEx3ouPAwveSJ6ZezNzMjMnax86GGOWj6rYY+7jvYeAlzPzm/NCjwH39n6+F/j50qdnjFkqFlP1djvwOWB/RDzX2/ZV4EHgJxHxeeAAcPeyZGiMWRKqYs/M3wAl8+7jVzNYZhb99NoigcovV/6p8rxrYyr/VHmkyreuedOKrmW1qpMr6HsGVEzdhwC6k67yy3fv3l2M7dy5U47ZdfFLdSy1e0BuuOGGYkyVHqvzr+vCjqrc23fQGdMIFrsxjWCxG9MIFrsxjWCxG9MIFrsxjTDQ7rJQLsFTXThBl34qW07ZG8oyAW3jvPbaa8WYKoncvHmzHFMdi7Kyui6iCNp2UtaRioFeiFLNkXrcmg2mzpO1a9cWY6qDbG3M1atXF2PKQlPnfM1C7LKYpK/sxjSCxW5MI1jsxjSCxW5MI1jsxjSCxW5MIwzUesvMYuVWrdOriqtqMNXFtLawY9eqLWW31KqZDh8+XIwpG1F1rT1y5IgcU1lAquGI6mgL2uoaHR3t9Li1RRaVzajmXnVlrZ2byi5VVWgqn1pFZumcV9ajr+zGNILFbkwjWOzGNILFbkwjWOzGNILFbkwjDNR6m52d5cyZMwvGapVZqnGkqgBSC1O8+uqrckxlY6hKMbWYn7KcQB+Lqq4aGxsrxmp2lbKHlHVZs96UJao4dOhQMabOA9AVart27SrG1LzXFsZUz8uWLVuKMfWc1ejSWNNXdmMawWI3phEsdmMawWI3phEsdmMawWI3phEWs4rrTRHx64h4KSJejIgv9bY/EBFTEfFc7+vO5U/XGNOVxfjsM8CXM/PZiBgFnomIX/Vi38rMry92sJmZGU6ePLlg7NixY3LfAwcOFGPT09PFmPJIax06S7mCLolU5Ym1cknl7Suuu+66Ykz56KBLLVWZbz+dck+fPl2MvfLKK8VYzfNWHW1L93gAnD9/vhir3S+gclLn9c0331yMqcUtoXz+qfNnMau4TgPTvZ/PRcTLwPbafsaYlcVVvWePiAngw8BTvU33RcTzEfFwRJT/pRpjhs6ixR4R64CfAvdn5lngO8AtwK3MXfm/UdhvT0Tsi4h9tVtijTHLx6LEHhGrmRP6DzLzZwCZeSwzZzPzEvBd4LaF9s3MvZk5mZmT6v2UMWZ5Wcyn8QE8BLycmd+ct33++j2fAV5Y+vSMMUvFYj6Nvx34HLA/Ip7rbfsqcE9E3Aok8DrwhWXIzxizRCzm0/jfAAt9zv/41Q528eJFpqamFoyprqoAv//974uxo0ePFmPKIlNlqrW4KqVUVo0qfwVdUqpKIpWlt3HjRjmmsgpVPuvXr5ePqxabVGW3qiutKicFbSOqfdVzpmw50Daish+V9VsrHy5Zreq89B10xjSCxW5MI1jsxjSCxW5MI1jsxjSCxW5MIwy0u+yFCxc4ePDggjG1MOHlfUso2+Ts2bPFmFrQELTtpGw5Zf/UxlQo60hZb/1Up6mYWvQRYOvWrcWYsrNmZmaKsVrVm6rSUx1Z+xlTzZFCndOqkhPKdqo6Dl/ZjWkEi92YRrDYjWkEi92YRrDYjWkEi92YRhio9aZQlU6gmyYqu0VRq6BSlpWyOFSjylrzR9UwUFmMytKrVb11XWCwn/lTCx6OjIwUYzWbS9me6nFVFV6t6k01slT2mqoKVJWcUH6+1TniK7sxjWCxG9MIFrsxjWCxG9MIFrsxjWCxG9MIFrsxjTBwn73kM6uFCUGXSyo/V5Wi1hbsU563KslVHWRrPrHyrtUcqHLT2tx27fSqPG3Q9z+ofZUHX+vOK7uriuNU902o0liA0dHRYkydQ+q5VscBZe9fne++shvTCBa7MY1gsRvTCBa7MY1gsRvTCBa7MY0QyhpY8sEiTgAH5m3aApTrQQeP89GstHxg5eU07Hx2ZuaCPuxAxf5/Bo/Yl5mTQ0vgCpyPZqXlAysvp5WWz3z8Mt6YRrDYjWmEYYt975DHvxLno1lp+cDKy2ml5fO/DPU9uzFmcAz7ym6MGRBDEXtE3BER/xURv4uIrwwjhyvyeT0i9kfEcxGxb0g5PBwRxyPihXnbxiLiVxHxau/7piHn80BETPXm6bmIuHOA+dwUEb+OiJci4sWI+FJv+1DmSOQztDmqMfCX8RFxDfAK8AngMPA0cE9mvjTQRP44p9eBycwcmj8aEX8FnAe+n5kf7G37R+BUZj7Y+6e4KTP/foj5PACcz8yvDyKHK/IZB8Yz89mIGAWeAT4N/C1DmCORz90MaY5qDOPKfhvwu8x8LTMvAD8C7hpCHiuKzHwSOHXF5ruAR3o/P8LcyTTMfIZGZk5n5rO9n88BLwPbGdIciXxWLMMQ+3bg0LzfDzP8SUrglxHxTETsGXIu89mWmZcX6j4KbBtmMj3ui4jney/zB/a2Yj4RMQF8GHiKFTBHV+QDK2COFsIf0M3xkcz8C+BvgC/2XsKuKHLu/dawrZPvALcAtwLTwDcGnUBErAN+CtyfmWfnx4YxRwvkM/Q5KjEMsU8BN837fUdv29DIzKne9+PAo8y91VgJHOu9N7z8HvH4MJPJzGOZOZuZl4DvMuB5iojVzAnrB5n5s97moc3RQvkMe44UwxD708DuiNgVEWuAzwKPDSEPACJipPcBCxExAnwSeEHvNTAeA+7t/Xwv8PMh5nJZTJf5DAOcp4gI4CHg5cz85rzQUOaolM8w56hKZg78C7iTuU/k/xv4h2HkMC+XPwN+2/t6cVj5AD9k7mXfReY+x/g8sBl4AngV+A9gbMj5/AuwH3ieOZGNDzCfjzD3Ev154Lne153DmiORz9DmqPblO+iMaQR/QGdMI1jsxjSCxW5MI1jsxjSCxW5MI1jsxjSCxW5MI1jsxjTC/wCcy/nlrpdzwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Bird after convolution:\")\n",
    "\n",
    "plt.imshow(output[0, 0].detach(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downsampling is common in multi-layer convolution, to ensure content \n",
    "initially larger than our kernels can be captured. This can be done using pooling\n",
    "or specially strided convolutions. We will focus on maxpooling, which has until now\n",
    "proved most effective.\n",
    "\"\"\"\n",
    "\n",
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Tanh()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Tanh()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Flatten()\n",
       "  (7): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (8): Tanh()\n",
       "  (9): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A convolutional model (basic CNN)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # End with fully connected layers\n",
    "    # Flatten required to transform the 8x8 image up to this point into 512x1 vector\n",
    "    # We can (and used to have to) forego this by subclassing nn.Module as below,\n",
    "    # which means abandoning the use of nn.Sequential\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(8 * 8 * 8, 32),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(32, 2)\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick parameter count\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative approach to Flatten: subclassing nn.Module.\n",
    "\n",
    "This is also often required for other operations which the provided modules don't supply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    The Net class is equivalent to the nn.Sequential model\n",
    "    we built earlier in terms of submodules; but by writing\n",
    "    the forward function explicitly, we can manipulate the\n",
    "    output of self.pool3 directly and call view on it to turn\n",
    "    it into a B √ó N vector (see out.view in the forward() function.\n",
    "    This is now allowed in an nn.Sequential model, for reasons.\n",
    "    \n",
    "    https://github.com/pytorch/pytorch/issues/2486\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functional API (torch.nn.functional) provides counterparts for nn.Modules\n",
    "# which have no internal state (e.g. parameters). This means modules which do not\n",
    "# require backprop can be placed in the forward() function\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A version of the prior Net model utilising the Functional API.\n",
    "    \n",
    "    It doesn't matter if we don't use it, but it can make the boilerplate\n",
    "    more concise as below.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        # tanh as a general-purpose scientific function remains in the torch namespaces\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1390, -0.0421]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does it run?\n",
    "\n",
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop definition\n",
    "\n",
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(\"{} Epoch {}, Training loss {}\".format(\n",
    "                datetime.datetime.now(),\n",
    "                epoch,\n",
    "                loss_train / len(train_loader)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 15:43:34.736920 Epoch 1, Training loss 0.5616403184119304\n",
      "2020-12-29 15:43:42.875628 Epoch 10, Training loss 0.33541767545946083\n",
      "2020-12-29 15:43:51.848313 Epoch 20, Training loss 0.2900612192928411\n",
      "2020-12-29 15:44:00.721711 Epoch 30, Training loss 0.2631163894181039\n",
      "2020-12-29 15:44:09.798966 Epoch 40, Training loss 0.24199788241534476\n",
      "2020-12-29 15:44:18.840328 Epoch 50, Training loss 0.22244340020000555\n",
      "2020-12-29 15:44:27.717401 Epoch 60, Training loss 0.20669520888358925\n",
      "2020-12-29 15:44:36.831292 Epoch 70, Training loss 0.19036743282132848\n",
      "2020-12-29 15:44:45.897086 Epoch 80, Training loss 0.1749586113698923\n",
      "2020-12-29 15:44:54.959023 Epoch 90, Training loss 0.16365510087673832\n",
      "2020-12-29 15:45:03.860084 Epoch 100, Training loss 0.14953561459377313\n"
     ]
    }
   ],
   "source": [
    "# Dataloader init and run\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.94\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Measuring accuracy on validation set\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading the model\n",
    "\n",
    "torch.save(model.state_dict(), data_path + \"birds_vs_airplanes_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(data_path + \"birds_vs_airplanes_v1.pt\"))\n",
    "\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "#¬†Training on GPU requires specification of which device you're using,\n",
    "# And casting the model to it.\n",
    "\n",
    "# NOTE: CUDA installation was messed up when this was written. TODO: run this on a working\n",
    "# GPU machine at some point.\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "    \n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            loss_train += loss.item()\n",
    "        \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 15:45:05.681524 Epoch 1, Training loss 0.5802819147990768\n",
      "2020-12-29 15:45:13.931716 Epoch 10, Training loss 0.33547416917837347\n",
      "2020-12-29 15:45:23.256881 Epoch 20, Training loss 0.2961105896029503\n",
      "2020-12-29 15:45:32.521926 Epoch 30, Training loss 0.2680801980814357\n",
      "2020-12-29 15:45:41.945774 Epoch 40, Training loss 0.24752296469393809\n",
      "2020-12-29 15:45:51.100881 Epoch 50, Training loss 0.23162111364732121\n",
      "2020-12-29 15:46:00.394841 Epoch 60, Training loss 0.21928325765261983\n",
      "2020-12-29 15:46:09.608865 Epoch 70, Training loss 0.20406374311561037\n",
      "2020-12-29 15:46:18.737461 Epoch 80, Training loss 0.19069370186063134\n",
      "2020-12-29 15:46:28.037005 Epoch 90, Training loss 0.18079545415320974\n",
      "2020-12-29 15:46:37.491792 Epoch 100, Training loss 0.1666493516913645\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    cifar2,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We have the option of casting a model trained on a certain device to another device\n",
    "before saving its state dict, but it's more concise (particularly if we're not sure\n",
    "what kind of device we'll next use it on), if we pass the current device used as map_location\n",
    "when later loading a saved state_dict.\n",
    "\"\"\"\n",
    "\n",
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(\n",
    "    torch.load(\n",
    "        data_path + \"birds_vs_airplanes_v1.pt\",\n",
    "        map_location=device # Here\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Changing the width of a NN is easy and can be parameterised as follows.\n",
    "\"\"\"\n",
    "\n",
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 //2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38386"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The numbers specifying channels and features for each layer are directly related to\n",
    "the number of parameters in a model; all other things being equal, they increase the\n",
    "capacity of the model.\n",
    "\n",
    "The greater the capacity, the more variability in the inputs the model will be able to\n",
    "manage; but at the same time, the more likely overfitting will be, since the model can\n",
    "use a greater number of parameters to memorize unessential aspects of the input.\n",
    "\"\"\"\n",
    "\n",
    "model = NetWidth()\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training loop with L2 regularization (weight decay)\n",
    "\n",
    "NOTE: PyTorch's SGD optimizer has a parameter weight_decay which\n",
    "performs L2 regularization during the model updates.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                    datetime.datetime.now(),\n",
    "                    epoch,\n",
    "                    loss_train / len(train_loader)\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetDropout(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example network with dropout\n",
    "\n",
    "NOTE: dropout is normally active during training, while during the evaluation of a\n",
    "trained model in production, dropout is bypassed or, equivalently, assigned a probability\n",
    "equal to zero. This is controlled through the train property of the Dropout\n",
    "module.\n",
    "\"\"\"\n",
    "\n",
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = slef.conv1_dropout(out)\n",
    "        out =  F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model = NetDropout()\n",
    "model.train() # Dropout modules active\n",
    "model.eval() # Dropout modules deactivated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetBatchNorm(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example network with batch norm\n",
    "\n",
    "Normalization is still done in eval mode, but the batch norm modules will stop refining\n",
    "their approximations of mean and stddev, based on inputs to the network, in order to do it.\n",
    "Instead these are frozen.\n",
    "\"\"\"\n",
    "\n",
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "        padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model = NetBatchNorm()\n",
    "model.train() # Outputs depend on the statistics of other inputs presented to the model\n",
    "model.eval() # Normalization parameters are fixed (running estimates of mean and stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example network with a skip connection\n",
    "\"\"\"\n",
    "\n",
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "        padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "        kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        # Identity mapping\n",
    "        # Below, output of the first 2 layers is added to the input of the following \n",
    "        # layers, this will encourage larger gradients in these later layers\n",
    "        # and prevent them from vanishing.\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining a very deep model dynamically with a for loop\n",
    "\"\"\"\n",
    "\n",
    "# Create a module subclass which provides the computation for one network block\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        # Bias is perhaps unnecessarily removed, as it would be cancelled out by batch norm\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        \n",
    "        # Kaiming initialization is used to mirror ResNet paper (by Kaiming He)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "    \n",
    "\n",
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        # Define a sequential section of n ResBlocks\n",
    "        # (notice this is using unpacking within nn.Sequential)\n",
    "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out) # look how elegant üëèüëèüëè\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "class ResBlockLargeKernel(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlockLargeKernel, self).__init__()\n",
    "        \n",
    "        # Bias is perhaps unnecessarily removed, as it would be cancelled out by batch norm\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=5, padding=2, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        \n",
    "        # Kaiming initialization is used to mirror ResNet paper (by Kaiming He)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "    \n",
    "\n",
    "class NetResDeepLargeKernel(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=5, padding=2)\n",
    "        # Define a sequential section of n ResBlocks\n",
    "        # (notice this is using unpacking within nn.Sequential)\n",
    "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlockLargeKernel(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out) # look how elegant üëèüëèüëè\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters of model with 3x3 kernels: 75810\n",
      "Num parameters of model with 5x5 kernels: 93730\n"
     ]
    }
   ],
   "source": [
    "old_model = NetResDeep()\n",
    "new_model = NetResDeepLargeKernel()\n",
    "\n",
    "print(f\"Num parameters of model with 3x3 kernels: {sum(p.numel() for p in old_model.parameters())}\")\n",
    "\n",
    "print(f\"Num parameters of model with 5x5 kernels: {sum(p.numel() for p in new_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 17:09:50.301053 Epoch 1, Training loss 0.5202728854432986\n",
      "2020-12-29 17:11:05.996886 Epoch 10, Training loss 0.25052246290027713\n",
      "2020-12-29 17:12:30.083811 Epoch 20, Training loss 0.16439602130157932\n",
      "2020-12-29 17:13:54.200700 Epoch 30, Training loss 0.11668118172485357\n",
      "2020-12-29 17:15:18.615371 Epoch 40, Training loss 0.06719226470205245\n",
      "2020-12-29 17:16:44.518801 Epoch 50, Training loss 0.025426735250267445\n",
      "2020-12-29 17:18:13.273122 Epoch 60, Training loss 0.01360794933435691\n",
      "2020-12-29 17:19:37.763340 Epoch 70, Training loss 0.008508479982691632\n",
      "2020-12-29 17:21:02.375609 Epoch 80, Training loss 0.024431639221929917\n",
      "2020-12-29 17:22:26.351341 Epoch 90, Training loss 0.006529858320281147\n",
      "2020-12-29 17:23:50.105568 Epoch 100, Training loss 0.0046856269884741845\n"
     ]
    }
   ],
   "source": [
    "# Train and validate both versions (question b)\n",
    "\n",
    "# Larger learning rate required due to network depth\n",
    "optimizer = optim.SGD(old_model.parameters(), lr=3e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = old_model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.99\n",
      "Accuracy val: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Valiation\n",
    "\n",
    "validate(old_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 17:55:16.457646 Epoch 1, Training loss 0.49891238645383507\n",
      "2020-12-29 17:57:23.593857 Epoch 10, Training loss 0.24797692894935608\n",
      "2020-12-29 17:59:44.538122 Epoch 20, Training loss 0.15284288824079142\n",
      "2020-12-29 18:02:05.006397 Epoch 30, Training loss 0.08948038256234804\n",
      "2020-12-29 18:04:24.977348 Epoch 40, Training loss 0.024353099316595846\n",
      "2020-12-29 18:06:44.196659 Epoch 50, Training loss 0.008615106650462053\n",
      "2020-12-29 18:09:03.822550 Epoch 60, Training loss 0.007486367485468175\n",
      "2020-12-29 18:11:23.168648 Epoch 70, Training loss 0.0031441627763609172\n",
      "2020-12-29 18:13:42.782172 Epoch 80, Training loss 0.002061145521765634\n",
      "2020-12-29 18:16:02.164690 Epoch 90, Training loss 0.011530046756112368\n",
      "2020-12-29 18:18:26.842340 Epoch 100, Training loss 0.006720295821755508\n"
     ]
    }
   ],
   "source": [
    "# For new model\n",
    "\n",
    "optimizer = optim.SGD(new_model.parameters(), lr=3e-3)\n",
    "\n",
    "# Batch size has to be reduced\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = new_model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 1.00\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "validate(new_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigger kernels have made overfitting worse! Makes sense, as for images of this size, we're probably learning fewer lower level features which generally demark the differences between birds and planes. Instead memorizing more features of the specific images in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suspect with a kernel of size 1 x 3, the kind of features you're able to detect would reduice to e.g. horizontal edges (vertical edges might be a challenge with no surrounding infromation) or extremely small features which appear as dots. But let's see how the model behaves..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockOneByThree(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlockOneByThree, self).__init__()\n",
    "        \n",
    "        # Bias is perhaps unnecessarily removed, as it would be cancelled out by batch norm\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=(1, 3), padding=(0, 1), bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        \n",
    "        # Kaiming initialization is used to mirror ResNet paper (by Kaiming He)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "    \n",
    "\n",
    "class NetResDeepOneByThree(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=(1, 3), padding=(0, 1))\n",
    "        # Define a sequential section of n ResBlocks\n",
    "        # (notice this is using unpacking within nn.Sequential)\n",
    "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlockOneByThree(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out) # look how elegant üëèüëèüëè\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_by_three_model = NetResDeepOneByThree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 18:46:08.264488 Epoch 1, Training loss 0.48734024603655385\n",
      "2020-12-29 18:47:05.159004 Epoch 10, Training loss 0.2887533436155623\n",
      "2020-12-29 18:48:08.751542 Epoch 20, Training loss 0.19536876175433968\n",
      "2020-12-29 18:49:11.647584 Epoch 30, Training loss 0.14445689989692845\n",
      "2020-12-29 18:50:14.593449 Epoch 40, Training loss 0.08576493622485999\n",
      "2020-12-29 18:51:17.477788 Epoch 50, Training loss 0.05021787306685357\n",
      "2020-12-29 18:52:20.504856 Epoch 60, Training loss 0.02959425278767279\n",
      "2020-12-29 18:53:23.333117 Epoch 70, Training loss 0.015177985424899561\n",
      "2020-12-29 18:54:26.456631 Epoch 80, Training loss 0.009661379759644817\n",
      "2020-12-29 18:55:29.561084 Epoch 90, Training loss 0.010214845245039671\n",
      "2020-12-29 18:56:32.508391 Epoch 100, Training loss 0.018478320112932402\n"
     ]
    }
   ],
   "source": [
    "# Train and eval\n",
    "\n",
    "optimizer = optim.SGD(one_by_three_model.parameters(), lr=3e-3)\n",
    "\n",
    "# Batch size has to be reduced\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = one_by_three_model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 1.00\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "validate(one_by_three_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, no great change. Overfitting, overfitting, overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpart1",
   "language": "python",
   "name": "venvpart1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
