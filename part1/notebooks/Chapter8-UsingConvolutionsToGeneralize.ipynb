{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note that the PyTorch nn module provides 3D convolutions for volumes or videos.\n",
    "For now though we'll stick to 2D examples.\n",
    "\n",
    "(3D convolution is used for operating on CT scan data in Part 2)\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(3, 16, kernel_size=3) # 3-channel, 3x3 kernel with 16 out channels\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Reinit cifar2\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "data_path = \"../data/cifar-10/\"\n",
    "\n",
    "\n",
    "class_names = [\n",
    "    \"Airplane\",\n",
    "    \"Automobile\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\"\n",
    "]\n",
    "\n",
    "# Add to dataset transforms\n",
    "transformed_cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "transformed_cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    ")\n",
    "   \n",
    "\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "cifar2 = [\n",
    "    (img, label_map[label])\n",
    "    for img, label in transformed_cifar10\n",
    "    if label in [0, 2]\n",
    "]\n",
    "\n",
    "\n",
    "cifar2_val = [\n",
    "    (img, label_map[label])\n",
    "    for img, label in transformed_cifar10_val\n",
    "    if label in [0, 2]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolution i/o example\n",
    "\n",
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0)) # requires dimensions B x C x H x W\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bird after convolution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Bird after convolution:\")\n",
    "\n",
    "plt.imshow(output[0, 0].detach(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downsampling is common in multi-layer convolution, to ensure content \n",
    "initially larger than our kernels can be captured. This can be done using pooling\n",
    "or specially strided convolutions. We will focus on maxpooling, which has until now\n",
    "proved most effective.\n",
    "\"\"\"\n",
    "\n",
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Tanh()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Tanh()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (8): Tanh()\n",
       "  (9): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A convolutional model (basic CNN)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # End with fully connected layers\n",
    "    # Flatten required to transform the 8x8 image up to this point into 512x1 vector\n",
    "    # We can (and used to have to) forego this by subclassing nn.Module as below,\n",
    "    # which means abandoning the use of nn.Sequential\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(8 * 8 * 8, 32),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(32, 2)\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick parameter count\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative approach to Flatten: subclassing nn.Module.\n",
    "\n",
    "This is also often required for other operations which the provided modules don't supply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    The Net class is equivalent to the nn.Sequential model\n",
    "    we built earlier in terms of submodules; but by writing\n",
    "    the forward function explicitly, we can manipulate the\n",
    "    output of self.pool3 directly and call view on it to turn\n",
    "    it into a B × N vector (see out.view in the forward() function.\n",
    "    This is now allowed in an nn.Sequential model, for reasons.\n",
    "    \n",
    "    https://github.com/pytorch/pytorch/issues/2486\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functional API (torch.nn.functional) provides counterparts for nn.Modules\n",
    "# which have no internal state (e.g. parameters). This means modules which do not\n",
    "# require backprop can be placed in the forward() function\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A version of the prior Net model utilising the Functional API.\n",
    "    \n",
    "    It doesn't matter if we don't use it, but it can make the boilerplate\n",
    "    more concise as below.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        # tanh as a general-purpose scientific function remains in the torch namespaces\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0882, -0.0735]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does it run?\n",
    "\n",
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop definition\n",
    "\n",
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(\"{} Epoch {}, Training loss {}\".format(\n",
    "                datetime.datetime.now(),\n",
    "                epoch,\n",
    "                loss_train / len(train_loader)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 19:45:34.865805 Epoch 1, Training loss 0.555843763860168\n",
      "2020-12-29 19:45:43.449843 Epoch 10, Training loss 0.3274434448996927\n",
      "2020-12-29 19:45:53.232652 Epoch 20, Training loss 0.2873439990506051\n",
      "2020-12-29 19:46:03.063304 Epoch 30, Training loss 0.26469072946317634\n",
      "2020-12-29 19:46:13.001821 Epoch 40, Training loss 0.24684049421624774\n",
      "2020-12-29 19:46:22.877061 Epoch 50, Training loss 0.23103074254883324\n",
      "2020-12-29 19:46:32.682065 Epoch 60, Training loss 0.2194479087925261\n",
      "2020-12-29 19:46:42.512427 Epoch 70, Training loss 0.20735006466223177\n",
      "2020-12-29 19:46:52.232926 Epoch 80, Training loss 0.19514886406101997\n",
      "2020-12-29 19:47:02.207825 Epoch 90, Training loss 0.1824719434378633\n",
      "2020-12-29 19:47:12.054592 Epoch 100, Training loss 0.1722698227330378\n"
     ]
    }
   ],
   "source": [
    "# Dataloader init and run\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.93\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Measuring accuracy on validation set\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading the model\n",
    "\n",
    "torch.save(model.state_dict(), data_path + \"birds_vs_airplanes_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(data_path + \"birds_vs_airplanes_v1.pt\"))\n",
    "\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "# Training on GPU requires specification of which device you're using,\n",
    "# And casting the model to it.\n",
    "\n",
    "# NOTE: CUDA installation was messed up when this was written. TODO: run this on a working\n",
    "# GPU machine at some point.\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "    \n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            loss_train += loss.item()\n",
    "        \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 19:47:15.953321 Epoch 1, Training loss 0.5631170367739003\n",
      "2020-12-29 19:47:19.235770 Epoch 10, Training loss 0.32288539884196726\n",
      "2020-12-29 19:47:22.803401 Epoch 20, Training loss 0.2858835739694583\n",
      "2020-12-29 19:47:26.398560 Epoch 30, Training loss 0.25981322675943375\n",
      "2020-12-29 19:47:30.030267 Epoch 40, Training loss 0.24628958058585027\n",
      "2020-12-29 19:47:33.625117 Epoch 50, Training loss 0.2291761056348017\n",
      "2020-12-29 19:47:37.236259 Epoch 60, Training loss 0.2106815562791126\n",
      "2020-12-29 19:47:40.846859 Epoch 70, Training loss 0.19772793233964095\n",
      "2020-12-29 19:47:44.453988 Epoch 80, Training loss 0.1800820046596846\n",
      "2020-12-29 19:47:48.050956 Epoch 90, Training loss 0.1671730545438399\n",
      "2020-12-29 19:47:51.639351 Epoch 100, Training loss 0.15245778948827915\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    cifar2,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We have the option of casting a model trained on a certain device to another device\n",
    "before saving its state dict, but it's more concise (particularly if we're not sure\n",
    "what kind of device we'll next use it on), if we pass the current device used as map_location\n",
    "when later loading a saved state_dict.\n",
    "\"\"\"\n",
    "\n",
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(\n",
    "    torch.load(\n",
    "        data_path + \"birds_vs_airplanes_v1.pt\",\n",
    "        map_location=device # Here\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Changing the width of a NN is easy and can be parameterised as follows.\n",
    "\"\"\"\n",
    "\n",
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 //2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38386"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The numbers specifying channels and features for each layer are directly related to\n",
    "the number of parameters in a model; all other things being equal, they increase the\n",
    "capacity of the model.\n",
    "\n",
    "The greater the capacity, the more variability in the inputs the model will be able to\n",
    "manage; but at the same time, the more likely overfitting will be, since the model can\n",
    "use a greater number of parameters to memorize unessential aspects of the input.\n",
    "\"\"\"\n",
    "\n",
    "model = NetWidth()\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training loop with L2 regularization (weight decay)\n",
    "\n",
    "NOTE: PyTorch's SGD optimizer has a parameter weight_decay which\n",
    "performs L2 regularization during the model updates.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                    datetime.datetime.now(),\n",
    "                    epoch,\n",
    "                    loss_train / len(train_loader)\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetDropout(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example network with dropout\n",
    "\n",
    "NOTE: dropout is normally active during training, while during the evaluation of a\n",
    "trained model in production, dropout is bypassed or, equivalently, assigned a probability\n",
    "equal to zero. This is controlled through the train property of the Dropout\n",
    "module.\n",
    "\"\"\"\n",
    "\n",
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = slef.conv1_dropout(out)\n",
    "        out =  F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model = NetDropout()\n",
    "model.train() # Dropout modules active\n",
    "model.eval() # Dropout modules deactivated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetBatchNorm(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example network with batch norm\n",
    "\n",
    "Normalization is still done in eval mode, but the batch norm modules will stop refining\n",
    "their approximations of mean and stddev, based on inputs to the network, in order to do it.\n",
    "Instead these are frozen.\n",
    "\"\"\"\n",
    "\n",
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "        padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model = NetBatchNorm()\n",
    "model.train() # Outputs depend on the statistics of other inputs presented to the model\n",
    "model.eval() # Normalization parameters are fixed (running estimates of mean and stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example network with a skip connection\n",
    "\"\"\"\n",
    "\n",
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "        padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "        kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        # Identity mapping\n",
    "        # Below, output of the first 2 layers is added to the input of the following \n",
    "        # layers, this will encourage larger gradients in these later layers\n",
    "        # and prevent them from vanishing.\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining a very deep model dynamically with a for loop\n",
    "\"\"\"\n",
    "\n",
    "# Create a module subclass which provides the computation for one network block\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        # Bias is perhaps unnecessarily removed, as it would be cancelled out by batch norm\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        \n",
    "        # Kaiming initialization is used to mirror ResNet paper (by Kaiming He)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "    \n",
    "\n",
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        # Define a sequential section of n ResBlocks\n",
    "        # (notice this is using unpacking within nn.Sequential)\n",
    "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out) # look how elegant 👏👏👏\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "class ResBlockLargeKernel(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlockLargeKernel, self).__init__()\n",
    "        \n",
    "        # Bias is perhaps unnecessarily removed, as it would be cancelled out by batch norm\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=5, padding=2, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        \n",
    "        # Kaiming initialization is used to mirror ResNet paper (by Kaiming He)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "    \n",
    "\n",
    "class NetResDeepLargeKernel(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=5, padding=2)\n",
    "        # Define a sequential section of n ResBlocks\n",
    "        # (notice this is using unpacking within nn.Sequential)\n",
    "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlockLargeKernel(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out) # look how elegant 👏👏👏\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters of model with 3x3 kernels: 75810\n",
      "Num parameters of model with 5x5 kernels: 93730\n"
     ]
    }
   ],
   "source": [
    "old_model = NetResDeep().to(device)\n",
    "new_model = NetResDeepLargeKernel().to(device)\n",
    "\n",
    "print(f\"Num parameters of model with 3x3 kernels: {sum(p.numel() for p in old_model.parameters())}\")\n",
    "\n",
    "print(f\"Num parameters of model with 5x5 kernels: {sum(p.numel() for p in new_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 19:49:47.260911 Epoch 1, Training loss 0.4840066734772579\n",
      "2020-12-29 19:49:55.950919 Epoch 10, Training loss 0.2476724088191986\n",
      "2020-12-29 19:50:05.584471 Epoch 20, Training loss 0.16053464515194013\n",
      "2020-12-29 19:50:15.311987 Epoch 30, Training loss 0.10122859655956554\n",
      "2020-12-29 19:50:25.016132 Epoch 40, Training loss 0.07332556972363193\n",
      "2020-12-29 19:50:34.763630 Epoch 50, Training loss 0.024704839349101494\n",
      "2020-12-29 19:50:44.466109 Epoch 60, Training loss 0.015397024164450872\n",
      "2020-12-29 19:50:54.197031 Epoch 70, Training loss 0.012258114873069772\n",
      "2020-12-29 19:51:03.942090 Epoch 80, Training loss 0.006410530967798061\n",
      "2020-12-29 19:51:13.716645 Epoch 90, Training loss 0.004056006446065512\n",
      "2020-12-29 19:51:23.569216 Epoch 100, Training loss 0.013415566305956169\n"
     ]
    }
   ],
   "source": [
    "# Train and validate both versions (question b)\n",
    "\n",
    "# Larger learning rate required due to network depth\n",
    "optimizer = optim.SGD(old_model.parameters(), lr=3e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = old_model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.96\n",
      "Accuracy val: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Valiation\n",
    "\n",
    "validate(old_model.cpu(), train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 19:53:19.184818 Epoch 1, Training loss 0.48638025875304153\n",
      "2020-12-29 19:53:28.701418 Epoch 10, Training loss 0.24220475530738284\n",
      "2020-12-29 19:53:39.250348 Epoch 20, Training loss 0.15255399959482205\n",
      "2020-12-29 19:53:49.730696 Epoch 30, Training loss 0.06734932794170395\n",
      "2020-12-29 19:54:00.215141 Epoch 40, Training loss 0.09858652578583758\n",
      "2020-12-29 19:54:10.735159 Epoch 50, Training loss 0.011422798099814896\n",
      "2020-12-29 19:54:21.215423 Epoch 60, Training loss 0.004517190350415363\n",
      "2020-12-29 19:54:31.677924 Epoch 70, Training loss 0.002597382390400051\n",
      "2020-12-29 19:54:42.119478 Epoch 80, Training loss 0.0018862591634480768\n",
      "2020-12-29 19:54:52.604809 Epoch 90, Training loss 0.001436693817050831\n",
      "2020-12-29 19:55:03.095017 Epoch 100, Training loss 0.004036840043256095\n"
     ]
    }
   ],
   "source": [
    "# For new model\n",
    "\n",
    "optimizer = optim.SGD(new_model.parameters(), lr=3e-3)\n",
    "\n",
    "# Batch size has to be reduced\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = new_model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 1.00\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "validate(new_model.cpu(), train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigger kernels have made overfitting worse! Makes sense, as for images of this size, we're probably learning fewer lower level features which generally demark the differences between birds and planes. Instead memorizing more features of the specific images in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suspect with a kernel of size 1 x 3, unless we change the stride used, performance will be comparable to a 3 x 3 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockOneByThree(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlockOneByThree, self).__init__()\n",
    "        \n",
    "        # Bias is perhaps unnecessarily removed, as it would be cancelled out by batch norm\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=(1, 3), padding=(0, 1), bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        \n",
    "        # Kaiming initialization is used to mirror ResNet paper (by Kaiming He)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "    \n",
    "\n",
    "class NetResDeepOneByThree(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=(1, 3), padding=(0, 1))\n",
    "        # Define a sequential section of n ResBlocks\n",
    "        # (notice this is using unpacking within nn.Sequential)\n",
    "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlockOneByThree(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out) # look how elegant 👏👏👏\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_by_three_model = NetResDeepOneByThree().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 19:55:09.861584 Epoch 1, Training loss 0.47905172455083034\n",
      "2020-12-29 19:55:18.878777 Epoch 10, Training loss 0.2714827323130741\n",
      "2020-12-29 19:55:28.830213 Epoch 20, Training loss 0.19067425024547394\n",
      "2020-12-29 19:55:38.771928 Epoch 30, Training loss 0.13264740494548516\n",
      "2020-12-29 19:55:48.758019 Epoch 40, Training loss 0.06971368054817816\n",
      "2020-12-29 19:55:58.760046 Epoch 50, Training loss 0.05315287613493812\n",
      "2020-12-29 19:56:08.727035 Epoch 60, Training loss 0.019715420533991924\n",
      "2020-12-29 19:56:18.761959 Epoch 70, Training loss 0.01563498870451834\n",
      "2020-12-29 19:56:28.870933 Epoch 80, Training loss 0.009897100804432941\n",
      "2020-12-29 19:56:38.923422 Epoch 90, Training loss 0.005938492508109802\n",
      "2020-12-29 19:56:48.903229 Epoch 100, Training loss 0.004672651847225324\n"
     ]
    }
   ],
   "source": [
    "# Train and eval\n",
    "\n",
    "optimizer = optim.SGD(one_by_three_model.parameters(), lr=3e-3)\n",
    "\n",
    "# Batch size has to be reduced\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = one_by_three_model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 1.00\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "validate(one_by_three_model.cpu(), train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, no great change. Overfitting, overfitting, overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpart1",
   "language": "python",
   "name": "venvpart1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
