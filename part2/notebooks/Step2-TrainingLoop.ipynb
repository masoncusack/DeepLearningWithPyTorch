{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for running command line version\n",
    "\n",
    "def run(app, *argv):\n",
    "    argv = list(argv)\n",
    "    argv.insert(0, '--num-workers=4')  # <1>\n",
    "    log.info(\"Running: {}({!r}).main()\".format(app, argv))\n",
    "    \n",
    "    app_cls = importstr(*app.rsplit('.', 1))  # <2>\n",
    "    app_cls(argv).main()\n",
    "    \n",
    "    log.info(\"Finished: {}.{!r}).main()\".format(app, argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "METRICS_LABEL_NDX=0\n",
    "METRICS_PRED_NDX=1\n",
    "METRICS_LOSS_NDX=2\n",
    "METRICS_SIZE = 3\n",
    "\n",
    "class LunaTrainingApp:\n",
    "    def __init__(self, sys_argv=None):\n",
    "        if sys_argv is None:\n",
    "            sys_argv = sys.argv[1:]\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--num-workers',\n",
    "            help='Number of worker processes for background data loading',\n",
    "            default=8,\n",
    "            type=int,\n",
    "        )\n",
    "        parser.add_argument('--batch-size',\n",
    "            help='Batch size to use for training',\n",
    "            default=32,\n",
    "            type=int,\n",
    "        )\n",
    "        parser.add_argument('--epochs',\n",
    "            help='Number of epochs to train for',\n",
    "            default=1,\n",
    "            type=int,\n",
    "        )\n",
    "\n",
    "        parser.add_argument('--tb-prefix',\n",
    "            default='p2ch11',\n",
    "            help=\"Data prefix to use for Tensorboard run. Defaults to chapter.\",\n",
    "        )\n",
    "\n",
    "        parser.add_argument('comment',\n",
    "            help=\"Comment suffix for Tensorboard run.\",\n",
    "            nargs='?',\n",
    "            default='dwlpt',\n",
    "        )\n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "\n",
    "        self.trn_writer = None\n",
    "        self.val_writer = None\n",
    "        self.totalTrainingSamples_count = 0\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "\n",
    "        self.model = self.initModel()\n",
    "        self.optimizer = self.initOptimizer()\n",
    "\n",
    "    def initModel(self):\n",
    "        model = LunaModel()\n",
    "        if self.use_cuda:\n",
    "            log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                model = nn.DataParallel(model)\n",
    "            model = model.to(self.device)\n",
    "        return model\n",
    "\n",
    "    def initOptimizer(self):\n",
    "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
    "        # return Adam(self.model.parameters())\n",
    "\n",
    "    def initTrainDl(self):\n",
    "        train_ds = LunaDataset(\n",
    "            val_stride=10,\n",
    "            isValSet_bool=False,\n",
    "        )\n",
    "\n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        train_dl = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        return train_dl\n",
    "\n",
    "    def initValDl(self):\n",
    "        val_ds = LunaDataset(\n",
    "            val_stride=10,\n",
    "            isValSet_bool=True,\n",
    "        )\n",
    "\n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        val_dl = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        return val_dl\n",
    "\n",
    "    def initTensorboardWriters(self):\n",
    "        if self.trn_writer is None:\n",
    "            log_dir = os.path.join('runs', self.cli_args.tb_prefix, self.time_str)\n",
    "\n",
    "            self.trn_writer = SummaryWriter(\n",
    "                log_dir=log_dir + '-trn_cls-' + self.cli_args.comment)\n",
    "            self.val_writer = SummaryWriter(\n",
    "                log_dir=log_dir + '-val_cls-' + self.cli_args.comment)\n",
    "\n",
    "\n",
    "    def main(self):\n",
    "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "\n",
    "        train_dl = self.initTrainDl()\n",
    "        val_dl = self.initValDl()\n",
    "\n",
    "        for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
    "\n",
    "            log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
    "                epoch_ndx,\n",
    "                self.cli_args.epochs,\n",
    "                len(train_dl),\n",
    "                len(val_dl),\n",
    "                self.cli_args.batch_size,\n",
    "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
    "            ))\n",
    "\n",
    "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
    "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "            valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
    "            self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "\n",
    "        if hasattr(self, 'trn_writer'):\n",
    "            self.trn_writer.close()\n",
    "            self.val_writer.close()\n",
    "            \n",
    "\n",
    "    def doTraining(self, epoch_ndx, train_dl):\n",
    "        self.model.train()\n",
    "        \n",
    "        # trnMetrics_g tensor to collect detailed per-class metrics during training\n",
    "        trnMetrics_g = torch.zeros(\n",
    "            METRICS_SIZE,\n",
    "            len(train_dl.dataset),\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            train_dl,\n",
    "            \"E{} Training\".format(epoch_ndx),\n",
    "            start_ndx=train_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, batch_tup in batch_iter:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss_var = self.computeBatchLoss(\n",
    "                batch_ndx,\n",
    "                batch_tup,\n",
    "                train_dl.batch_size,\n",
    "                trnMetrics_g\n",
    "            )\n",
    "\n",
    "            loss_var.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # # This is for adding the model graph to TensorBoard.\n",
    "            # if epoch_ndx == 1 and batch_ndx == 0:\n",
    "            #     with torch.no_grad():\n",
    "            #         model = LunaModel()\n",
    "            #         self.trn_writer.add_graph(model, batch_tup[0], verbose=True)\n",
    "            #         self.trn_writer.close()\n",
    "\n",
    "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
    "\n",
    "        return trnMetrics_g.to('cpu')\n",
    "\n",
    "\n",
    "    def doValidation(self, epoch_ndx, val_dl):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            valMetrics_g = torch.zeros(\n",
    "                METRICS_SIZE,\n",
    "                len(val_dl.dataset),\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "            batch_iter = enumerateWithEstimate(\n",
    "                val_dl,\n",
    "                \"E{} Validation \".format(epoch_ndx),\n",
    "                start_ndx=val_dl.num_workers,\n",
    "            )\n",
    "            for batch_ndx, batch_tup in batch_iter:\n",
    "                self.computeBatchLoss(\n",
    "                    batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
    "\n",
    "        return valMetrics_g.to('cpu')\n",
    "\n",
    "\n",
    "\n",
    "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
    "        \n",
    "        \"\"\"\n",
    "        Called by both the training and validation loops, this computes the\n",
    "        loss over a batch of samples. It also computes and records per-sample\n",
    "        information about the output the model is producing. This lets us compute\n",
    "        things like the percentage of correct answers per class, which allows us\n",
    "        to hone in on areas where our model is having difficulty.\n",
    "        \"\"\"\n",
    "        \n",
    "        input_t, label_t, _series_list, _center_list = batch_tup\n",
    "\n",
    "        input_g = input_t.to(self.device, non_blocking=True)\n",
    "        label_g = label_t.to(self.device, non_blocking=True)\n",
    "\n",
    "        logits_g, probability_g = self.model(input_g)\n",
    "\n",
    "        loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "        loss_g = loss_func(\n",
    "            logits_g,\n",
    "            label_g[:,1],\n",
    "        )\n",
    "        start_ndx = batch_ndx * batch_size\n",
    "        end_ndx = start_ndx + label_t.size(0)\n",
    "\n",
    "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = \\\n",
    "            label_g[:,1].detach()\n",
    "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = \\\n",
    "            probability_g[:,1].detach()\n",
    "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = \\\n",
    "            loss_g.detach()\n",
    "\n",
    "        return loss_g.mean()\n",
    "\n",
    "\n",
    "    def logMetrics(\n",
    "            self,\n",
    "            epoch_ndx,\n",
    "            mode_str,\n",
    "            metrics_t,\n",
    "            classificationThreshold=0.5,\n",
    "    ):\n",
    "        self.initTensorboardWriters()\n",
    "        log.info(\"E{} {}\".format(\n",
    "            epoch_ndx,\n",
    "            type(self).__name__,\n",
    "        ))\n",
    "\n",
    "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] <= classificationThreshold\n",
    "        negPred_mask = metrics_t[METRICS_PRED_NDX] <= classificationThreshold\n",
    "\n",
    "        posLabel_mask = ~negLabel_mask\n",
    "        posPred_mask = ~negPred_mask\n",
    "\n",
    "        neg_count = int(negLabel_mask.sum())\n",
    "        pos_count = int(posLabel_mask.sum())\n",
    "\n",
    "        neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "        pos_correct = int((posLabel_mask & posPred_mask).sum())\n",
    "\n",
    "        metrics_dict = {}\n",
    "        metrics_dict['loss/all'] = \\\n",
    "            metrics_t[METRICS_LOSS_NDX].mean()\n",
    "        metrics_dict['loss/neg'] = \\\n",
    "            metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
    "        metrics_dict['loss/pos'] = \\\n",
    "            metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
    "\n",
    "        metrics_dict['correct/all'] = (pos_correct + neg_correct) \\\n",
    "            / np.float32(metrics_t.shape[1]) * 100\n",
    "        metrics_dict['correct/neg'] = neg_correct / np.float32(neg_count) * 100\n",
    "        metrics_dict['correct/pos'] = pos_correct / np.float32(pos_count) * 100\n",
    "\n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/all:.4f} loss, \"\n",
    "                 + \"{correct/all:-5.1f}% correct, \"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/neg:.4f} loss, \"\n",
    "                 + \"{correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + '_neg',\n",
    "                neg_correct=neg_correct,\n",
    "                neg_count=neg_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/pos:.4f} loss, \"\n",
    "                 + \"{correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + '_pos',\n",
    "                pos_correct=pos_correct,\n",
    "                pos_count=pos_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        writer = getattr(self, mode_str + '_writer')\n",
    "\n",
    "        for key, value in metrics_dict.items():\n",
    "            writer.add_scalar(key, value, self.totalTrainingSamples_count)\n",
    "\n",
    "        writer.add_pr_curve(\n",
    "            'pr',\n",
    "            metrics_t[METRICS_LABEL_NDX],\n",
    "            metrics_t[METRICS_PRED_NDX],\n",
    "            self.totalTrainingSamples_count,\n",
    "        )\n",
    "\n",
    "        bins = [x/50.0 for x in range(51)]\n",
    "\n",
    "        negHist_mask = negLabel_mask & (metrics_t[METRICS_PRED_NDX] > 0.01)\n",
    "        posHist_mask = posLabel_mask & (metrics_t[METRICS_PRED_NDX] < 0.99)\n",
    "\n",
    "        if negHist_mask.any():\n",
    "            writer.add_histogram(\n",
    "                'is_neg',\n",
    "                metrics_t[METRICS_PRED_NDX, negHist_mask],\n",
    "                self.totalTrainingSamples_count,\n",
    "                bins=bins,\n",
    "            )\n",
    "        if posHist_mask.any():\n",
    "            writer.add_histogram(\n",
    "                'is_pos',\n",
    "                metrics_t[METRICS_PRED_NDX, posHist_mask],\n",
    "                self.totalTrainingSamples_count,\n",
    "                bins=bins,\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LunaDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "import os\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "class LunaDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 val_stride=0,\n",
    "                 isValSet_bool=None,\n",
    "                 series_uid=None,\n",
    "            ):\n",
    "        self.candidateInfo_list = copy.copy(getCandidateInfoList())\n",
    "\n",
    "        if series_uid:\n",
    "            self.candidateInfo_list = [\n",
    "                x for x in self.candidateInfo_list if x.series_uid == series_uid\n",
    "            ]\n",
    "\n",
    "        # Train/validation split\n",
    "        if isValSet_bool:\n",
    "            assert val_stride > 0, val_stride\n",
    "            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
    "            assert self.candidateInfo_list\n",
    "        elif val_stride > 0:\n",
    "            del self.candidateInfo_list[::val_stride]\n",
    "            assert self.candidateInfo_list\n",
    "\n",
    "        log.info(\"{!r}: {} {} samples\".format(\n",
    "            self,\n",
    "            len(self.candidateInfo_list),\n",
    "            \"validation\" if isValSet_bool else \"training\",\n",
    "        ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.candidateInfo_list)\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        candidateInfo_tup = self.candidateInfo_list[ndx]\n",
    "        width_irc = (32, 48, 48)\n",
    "\n",
    "        candidate_a, center_irc = getCtRawCandidate(\n",
    "            candidateInfo_tup.series_uid,\n",
    "            candidateInfo_tup.center_xyz,\n",
    "            width_irc,\n",
    "        )\n",
    "\n",
    "        candidate_t = torch.from_numpy(candidate_a)\n",
    "        candidate_t = candidate_t.to(torch.float32)\n",
    "        candidate_t = candidate_t.unsqueeze(0)\n",
    "\n",
    "        pos_t = torch.tensor([\n",
    "                not candidateInfo_tup.isNodule_bool,\n",
    "                candidateInfo_tup.isNodule_bool\n",
    "            ],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "\n",
    "        return ( # training example\n",
    "            candidate_t,\n",
    "            pos_t,\n",
    "            candidateInfo_tup.series_uid,\n",
    "            torch.tensor(center_irc),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataloader creation\n",
    "\n",
    "def initTrainDl(self):\n",
    "    train_ds = LunaDataset(\n",
    "        val_stride=10,\n",
    "        isValSet_bool=False,\n",
    "    )\n",
    "    batch_size = self.cli_args.batch_size\n",
    "    if self.use_cuda:\n",
    "        batch_size *= torch.cuda.device_count()\n",
    "    \n",
    "    train_dl = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=self.cli_args.num_workers,\n",
    "        pin_memory=self.use_cuda,\n",
    "    )\n",
    "    \n",
    "    return train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodule classification model definition\n",
    "import torch.nn as nn\n",
    "\n",
    "class LunaBlock(nn.Module):\n",
    "    def __init__(self, in_channels, conv_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels, conv_channels, kernel_size=3, padding=1, bias=True\n",
    "        )\n",
    "        \n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(\n",
    "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True\n",
    "        )\n",
    "        \n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        block_out = self.conv1(input_batch)\n",
    "        block_out = self.relu1(block_out)\n",
    "        block_out = self.conv2(block_out)\n",
    "        block_out = self.relu2(block_out)\n",
    "        \n",
    "        return self.maxpool(block_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "class LunaModel(nn.Module):\n",
    "    def __init__(self, in_channels=1, conv_channels=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Tail\n",
    "        self.tail_batchnorm = nn.BatchNorm3d(1)\n",
    "        \n",
    "        # Backbone\n",
    "        self.block1 = LunaBlock(in_channels, conv_channels)\n",
    "        self.block2 = LunaBlock(conv_channels, conv_channels * 2)\n",
    "        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4)\n",
    "        self.block4 = LunaBlock(conv_channels * 4, conv_channels * 8)\n",
    "        \n",
    "        # Head\n",
    "        self.head_linear = nn.Linear(1152, 2)\n",
    "        self.head_softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        \n",
    "        bn_output = self.tail_batchnorm(input_batch)\n",
    "        block_out = self.block1(bn_output)\n",
    "        block_out = self.block2(block_out)\n",
    "        block_out = self.block3(block_out)\n",
    "        block_out = self.block4(block_out)\n",
    "        # Fully connected layers require flattened inputs\n",
    "        conv_flat = block_out.view(\n",
    "            block_out.size(0),\n",
    "            -1,\n",
    "        )\n",
    "        \n",
    "        linear_output = self.head_linear(conv_flat)\n",
    "        \n",
    "        return linear_output, self.head_softmax(linear_output)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Because PyTorch's default weight initalization is not the best.\n",
    "        \"\"\"\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if type(m) in {\n",
    "            nn.Linear,\n",
    "            nn.Conv3d,\n",
    "            }:\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n",
    "                )\n",
    "                \n",
    "            if m.bias is not None:\n",
    "                fan_in, fan_out = \\\n",
    "                    nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
    "                bound = 1 / math.sqrt(fan_out)\n",
    "                nn.init.normal_(m.bias, -bound, bound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpart2",
   "language": "python",
   "name": "venvpart2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
